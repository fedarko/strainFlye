# Utilities for strainFlye fdr.


import os
import tempfile
import subprocess
import pysam
import pysamstats
import skbio
import numpy as np
import pandas as pd
from math import floor
from statistics import mean
from collections import defaultdict
from strainflye import fasta_utils, call_utils, misc_utils, bcf_utils, config
from .errors import ParameterError, SequencingDataError, WeirdError


def check_decoy_selection(diversity_indices, decoy_contig):
    """Checks that only one of (diversity index file, decoy contig) is given.

    Parameters
    ----------
    diversity_indices: str or None
        If a str, this should be a filepath to a TSV file representing
        diversity index info.

    decoy_contig: str or None
        If a str, this should be the name of a contig described in the
        BCF file.

    Returns
    -------
    selection_type: str
        Will be "DI" if only diversity_indices is not None, and will be "DC" if
        only decoy_contig is not None.

    Raises
    ------
    ParameterError
        - If diversity_indices and decoy_contig are both not None
        - If diversity_indices and decoy_contig are both None
    """
    di = diversity_indices is not None
    dc = decoy_contig is not None

    if di:
        if dc:
            raise ParameterError(
                "Both the diversity indices file and a decoy contig are "
                "specified. These options are mutually exclusive."
            )
        else:
            return "DI"
    else:
        if dc:
            return "DC"
        else:
            raise ParameterError(
                "Either the diversity indices file or a decoy contig must be "
                "specified."
            )


def normalize_series(in_series):
    """Converts a series to values in the range [0, 1].

    Parameters
    ----------
    in_series: pd.Series
        We assume that this does not contain any nan values.

    Returns
    -------
    None or pd.Series
        If the minimum and maximum of in_series are identical, this will just
        return None. (In this case, we can't scale values, because the
        denominator we use when converting a value (max - min) is zero.)

        If the minimum and maximum are not identical (which should usually
        be the case with diversity indices, hopefully...) then this will return
        a pd.Series with the same index as in_series, but with each entry
        scaled to within the range [0, 1] (such that the min value in in_series
        is set to 0, the max is set to 1, and everything else is in between).
    """
    # Small TODO: in theory, it'd be faster to combine the
    # computation of min and max into a single pass over the values
    # (see e.g. https://stackoverflow.com/q/12200580) but this
    # probably won't be a performance bottleneck so I'm not gonna
    # bother for now
    min_val = min(in_series)
    max_val = max(in_series)
    if min_val == max_val:
        return None
    else:
        # Use pandas' vectorization to apply linear interpolation
        # across all diversity indices in this Series
        return (in_series - min_val) / (max_val - min_val)


def autoselect_decoy(diversity_indices, min_len, min_avg_cov, fancylog):
    """Attempts to select a good decoy contig based on diversity index data.

    There are lots of ways to implement this, so here we just stick with
    something simple that combines the diversity index information from
    multiple thresholds:

    1. Filter to all contigs whose lengths and average coverages meet the
       specified thresholds. The number of "passing" contigs is C. If C = 1,
       select this contig; if C = 0, raise an error.

    2. For each of the D diversity index columns provided in the file (where at
       least two contigs have defined diversity indices), compute the minimum
       and maximum diversity index in this column. Assign each contig a score
       for this column in [0, 1] using linear interpolation: the contig with
       the lowest diversity index gets a score of 0, the contig with the
       highest gets a score of 1, and everything else is scaled in between.
       If a contig has an undefined diversity index in such a column, set its
       score for this column to 1.

    3. For each of the C contigs, sum scores across all of the D diversity
       index columns. Select the contig with the lowest score sum. Break ties
       arbitrarily.

    Parameters
    ----------
    diversity_indices: str
        Filepath to a TSV file containing diversity index info, generated by
        one of strainFlye call's subcommands. In addition to diversity index
        values, this also includes length and average coverage information
        for each contig in the file (this will help a lot, since computing
        these values if we don't already have them is time-consuming or at
        the very least annoying).

    min_len: int
        In order for a contig to be selected as the decoy, its length must be
        at least this.

    min_avg_cov: float
        In order for a contig to be selected as the decoy, its average coverage
        must be at least this.

    fancylog: function
        Logging function.

    Returns
    -------
    decoy_contig: str
        The name of the decoy contig we find.

    Raises
    ------
    ParameterError
        If the diversity index file:
        - Describes < 2 contigs (should have already been caught during align,
          but you never know)
        - Doesn't have Length or AverageCoverage columns

    SequencingDataError
        - If none of the contigs in the diversity index file pass the length
          and average coverage thresholds.
        - If none of the diversity index columns has at least two "passing"
          contigs with defined and distinct diversity indices in this column.
    """
    # We require that the diversity indices file describes at least two
    # contigs, so that -- once we select one contig as a decoy -- we still have
    # at least one target contig left!
    di = misc_utils.load_and_sanity_check_diversity_indices(
        diversity_indices, min_num_contigs=2
    )

    # Filter to contigs that pass both the length and coverage thresholds.
    # https://stackoverflow.com/a/13616382
    passing_di = di[
        (di["Length"] >= min_len) & (di["AverageCoverage"] >= min_avg_cov)
    ]
    passing_contigs = passing_di.index
    num_passing_contigs = len(passing_contigs)
    # it isn't clear how much precision min_avg_cov has, so we don't impose any
    # limit on how many digits it goes out to when printing it out. let's let
    # python handle this one
    check_str = (
        f"the min length \u2265 {min_len:,} and min average cov \u2265 "
        f"{min_avg_cov:,}x checks"
    )
    if num_passing_contigs == 0:
        raise SequencingDataError(f"No contigs pass {check_str}.")
    elif num_passing_contigs == 1:
        # Arguably, we could raise an error here -- but we know at this point
        # that there are >= 2 contigs in the file (and this is just the only
        # one that passes the length and coverage thresholds). So we may as
        # well select this contig, albeit after giving a warning.
        fancylog(
            f"Warning: Only one contig passes {check_str}. Selecting it.",
            prefix="",
        )
        return passing_contigs[0]

    # Actually start scoring contigs based on their diversity indices.
    contig2score = defaultdict(int)
    # Diversity index columns where there are at least two "passing" contigs
    # that have defined (non-NA) diversity indices
    good_di_cols = []
    for di_col in passing_di.columns:

        # ignore non-diversity-index columns
        if di_col.startswith(config.DI_PREF):
            di_vals = passing_di[di_col]

            # Ignore diversity index columns where less than two contigs have
            # defined diversity indices, since these don't mean much for our
            # score computation (at least as currently defined)
            finite_di_vals = di_vals[~di_vals.isna()]
            if len(finite_di_vals.index) >= 2:

                scores = normalize_series(finite_di_vals)
                # normalize_series() will return None if the min and max value
                # in finite_di_vals are identical. In this case, we can't
                # generate meaningful scores, so we just move on.
                if scores is not None:
                    good_di_cols.append(di_col)

                    # Update scores.
                    for passing_contig in passing_di.index:
                        if passing_contig in scores:
                            contig2score[passing_contig] += scores[
                                passing_contig
                            ]
                        else:
                            # Penalize this contig for not having a defined
                            # diversity index in this column: give it the max
                            # possible score
                            contig2score[passing_contig] += 1

    if len(good_di_cols) == 0:
        raise SequencingDataError(
            "No diversity index column has at least two contigs that (1) pass "
            f"{check_str} and (2) have defined and distinct diversity indices "
            "in this column."
        )
    # Find the passing contig with the lowest total score:
    # https://stackoverflow.com/a/3282904
    lowest_score_contig = min(passing_contigs, key=contig2score.get)
    return lowest_score_contig


def extract_and_verify_thresh_vals(thresh_vals):
    if thresh_vals.step != 1:
        raise ParameterError("thresh_vals must use a step size of 1.")
    if len(thresh_vals) <= 0:
        raise ParameterError("thresh_vals must have a positive length.")
    if thresh_vals.start <= 0 or thresh_vals.stop <= 0:
        raise ParameterError("thresh_vals' start and stop must be positive.")

    # We can just infer the "indisputable" mutation value from thresh_vals as
    # the value just above the max thresh_vals entry
    high_val = thresh_vals[-1] + 1
    min_val = thresh_vals[0]

    return min_val, high_val


def update_number_of_mutations_at_thresholds(
    num_muts, thresh_type, min_val, high_val, mut
):
    # AAD is technically a tuple since it's defined once for every alt
    # allele, but r/n strainflye call only produces max one alt allele
    # per mutation. So it's a tuple with 1 element (at least for now).
    alt_pos = mut.info.get("AAD")[0]
    cov_pos = mut.info.get("MDP")

    if thresh_type == "p":
        max_passing_val = floor((10000 * alt_pos) / cov_pos)
    else:
        max_passing_val = alt_pos

    # Don't count "indisputable" mutations towards mutation rates
    if max_passing_val < high_val:
        # NOTE: This is already more optimized than the analysis
        # notebooks, but I think it could still be made faster. Maybe just
        # increment a single value (corresponding to the max passing p/r),
        # and then do everything at the end after seeing all mutations in
        # one pass? Doesn't seem like a huge bottleneck tho.
        num_vals_to_update = max_passing_val - min_val + 1
        for i in range(num_vals_to_update):
            num_muts[i] += 1


def compute_number_of_mutations_in_contig(
    bcf_obj, thresh_type, thresh_vals, contig, pos_to_consider=None
):
    """Counts mutations at certain p or r thresholds in a contig.

    This function is designed to be useful for either decoy or target contigs.

    We perform some sanity checking on thresh_vals, but we'll assume that
    the other parameters are well-formed (e.g. thresh_type is either "p" or
    "r", contig is in bcf_obj, etc.)

    Parameters
    ----------
    bcf_obj: pysam.VariantFile
        Object describing a BCF file produced by strainFlye's naive calling.

    thresh_type: str
        Either "p" or "r", depending on which type of mutations were called in
        bcf_obj.

    thresh_vals: range
        Range of values of p or r (depending on thresh_type) at which to
        count mutations in this contig. Must use a step size of 1.
        If a mutation is a mutation for a value of p or r larger than the
        maximum p or r value here (i.e. it's "indisputable"), it will not be
        counted.

    contig: str
        Name of a contig for which mutation rates will be computed.

    pos_to_consider: set or None
        If this is None, then consider mutations at *all* positions in the
        contig.

        If this is a set, then we assume that this set describes 1-indexed
        positions in this contig; we will then only count mutations occurring
        in positions given in this set.

    Returns
    -------
    num_muts: list of int
        List with the same length as thresh_vals. The i-th value in this list
        describes the number of called mutations for the i-th threshold in
        thresh_vals.

    Raises
    ------
    ParameterError
        If thresh_vals doesn't use a step size of 1.
        If len(thresh_vals) is zero.
        If either the stop or start of thresh_vals are zero or below.

        (None of these should happen in practice, but you never know...)
    """
    min_val, high_val = extract_and_verify_thresh_vals(thresh_vals)
    # For each threshold value, keep track of how many mutations we've seen at
    # this threshold.
    num_muts = [0] * len(thresh_vals)

    for mut in bcf_obj.fetch(contig):

        # only count this mutation if (1) we are considering all positions, or
        # (2) we are only considering some positions and this is one of them.
        # (this abuses short circuiting)
        if pos_to_consider is None or mut.pos in pos_to_consider:
            update_number_of_mutations_at_thresholds(
                num_muts, thresh_type, min_val, high_val, mut
            )
    return num_muts


def compute_any_mutation_decoy_contig_mut_rates(
    bcf_obj, thresh_type, thresh_vals, contig, pos_to_consider=None
):
    """Computes mutation rates (any type) for positions in a contig.

    This is designed for the "Full" option, in which we consider every position
    in the contig as part of the decoy; or the "CP2" option, in which we just
    consider single-gene CP2 positions in the contig as part of the decoy.

    In either case, we consider *any* mutation at these positions to count
    towards the decoy -- we don't impose restrictions on these mutations (see:
    the nonsense, nonsyn, transversion stuff).

    Parameters
    ----------
    bcf_obj: pysam.VariantFile
        Object describing a BCF file produced by strainFlye's naive calling.

    thresh_type: str
        Either "p" or "r", depending on which type of mutations were called in
        bcf_obj.

    thresh_vals: range
        Range of values of p or r (depending on thresh_type) at which to
        compute mutation rates for this contig. Must use a step size of 1.
        If a mutation is a mutation for a value of p or r larger than the
        maximum p or r value here (i.e. it's "indisputable"), it will not be
        included in the mutation rate computation.

    contig: str
        Decoy contig name.

    pos_to_consider: set or None
        If this is None, then consider mutations at *all* positions in the
        contig.

        If this is a set, then we assume that this set describes 1-indexed
        positions in this contig; we will then only count mutations occurring
        in positions given in this set.

    Returns
    -------
    mut_rates: list of float
        Mutation rates for each threshold value in thresh_vals.
    """
    num_muts = compute_number_of_mutations_in_contig(
        bcf_obj,
        thresh_type,
        thresh_vals,
        contig,
        pos_to_consider=pos_to_consider,
    )

    if pos_to_consider is None:
        pos_len = bcf_obj.header.contigs[contig].length
    else:
        pos_len = len(pos_to_consider)

    denominator = 3 * pos_len
    return [n / denominator for n in num_muts]


def get_mutation_types(mutation_types):
    """Parses and checks a list of mutation types.

    Parameters
    ----------
    mutation_types: list of str
        Types of mutation we will specifically count towards a decoy contig.

    Returns
    -------
    tv, nonsyn, nonsense: bool, bool, bool
        These bools represent whether or not we limit our focus to each type of
        mutation: transversions, nonsynonymous, and nonsense mutations,
        respectively.

    Raises
    ------
    ParameterError
        - If mutation_types is empty
        - If any of the entries in mutation_types are unrecognized
        - If both Nonsyn and Nonsense are specified in mutation_types
    """
    if len(mutation_types) == 0:
        raise ParameterError("mutation_types must be nonempty.")

    nonsyn = False
    nonsense = False
    tv = False
    for mt in mutation_types:
        if mt == "Nonsyn":
            nonsyn = True
        elif mt == "Nonsense":
            nonsense = True
        elif mt == "Tv":
            tv = True
        else:
            raise ParameterError(f"Unrecognized mutation type: {mt}")

    if nonsyn and nonsense:
        raise ParameterError(
            "No need to specify both Nonsyn and Nonsense: just say Nonsense."
        )
    return tv, nonsyn, nonsense


def get_mutation_types_for_cp(codon, cp, alt_nt):
    """Classifies a mutation at a position as synonymous and non-nonsense.

    Parameters
    ----------
    codon: str
        DNA codon we are considering.

    pos: int
        One-indexed position within the codon that we are mutating. Should be
        one of 1, 2, or 3.

    alt_nt: str
        Alternate nucleotide to which we will try mutating the pos-th position
        in this codon.

    Returns
    -------
    (si, nnsi): (bool, bool)
        si will be True if this is a synonymous mutation and False otherwise.

        If the codon is one of the 61 sense codons, then nnsi will be True if
        this is a non-nonsense mutation and False otherwise.

        If the codon is one of the 3 stop codons, then nnsi will be None.

    Raises
    ------
    WeirdError
        If alt_nt is the same nucleotide as that given by codon[cp - 1].
    """
    pos = cp - 1
    if alt_nt == codon[pos]:
        raise WeirdError(
            f"In codon {codon}, trying to mutate CP {cp} into itself?"
        )
    alt_codon = codon[:pos] + alt_nt + codon[pos + 1 :]
    aa1 = str(skbio.DNA(codon).translate())
    aa2 = str(skbio.DNA(alt_codon).translate())

    si = False
    nnsi = None

    if aa1 == aa2:
        si = True
        if aa1 != "*":
            nnsi = True
    else:
        if aa1 != "*":
            if aa2 == "*":
                nnsi = False
            else:
                nnsi = True
    return (si, nnsi)


class CodonPositionMutationCounts(object):
    """Represents the number of possible mutations at a CP in a codon."""

    def __init__(self, codon, cp):
        """Initializes the object.

        Parameters
        ----------
        codon: str
            The three-character DNA codon we are considering. Each character
            should be one of "A", "C", "G", or "T".

        cp: int
            One of the codon positions (CPs) within this codon. This is
            one-indexed, so this should be one of 1, 2, or 3.

        Raises
        ------
        WeirdError
            If codon does not have length 3 and contain only {A, C, G, T}.
            If cp is not one of 1, 2, or 3.
        """
        if len(codon) != 3:
            raise WeirdError("Codon must be exactly 3 nt long")

        for nt in codon:
            if nt not in ["A", "C", "G", "T"]:
                raise WeirdError("Codon should only contain {A, C, G, T}")

        if cp not in (1, 2, 3):
            raise WeirdError("CP must be one of 1, 2, or 3")

        self.codon = codon
        self.cp = cp

        self.nt = self.codon[cp - 1]
        self.aa = str(skbio.DNA(self.codon).translate())
        self.in_sense_codon = self.aa != "*"

        self.si = 0
        self.ni = 0
        self.si_tv = 0
        self.ni_tv = 0
        if self.in_sense_codon:
            self.nnsi = 0
            self.nsi = 0
            self.nnsi_tv = 0
            self.nsi_tv = 0
        else:
            # I want things that try to use these in place of ordinary numbers
            # to fail explicitly, rather than silently do weird stuff with
            # zeros
            self.nnsi = None
            self.nsi = None
            self.nnsi_tv = None
            self.nsi_tv = None

        self._fill_in_counts()
        self._sanity_check_final()

    def _fill_in_counts(self):
        """Fills in the mutation type counts for this object."""
        for alt_nt in set("ACGT") - set(self.nt):

            is_si, is_nnsi = get_mutation_types_for_cp(
                self.codon, self.cp, alt_nt
            )
            tv = is_transversion(self.nt, alt_nt)

            if is_si:
                self.si += 1
                if tv:
                    self.si_tv += 1
            else:
                self.ni += 1
                if tv:
                    self.ni_tv += 1

            if self.in_sense_codon:
                if is_nnsi:
                    self.nnsi += 1
                    if tv:
                        self.nnsi_tv += 1
                else:
                    self.nsi += 1
                    if tv:
                        self.nsi_tv += 1

    def _sanity_check_final(self):
        """Checks that the counts look good after recording possible mutations.

        Raises
        ------
        WeirdError
            If Si + Ni != 3.
            If self.in_sense_codon is True, and NNSi + NSi != 3.
            If self.in_sense_codon is False, and either NNSi or NSi isn't None.

            ...And versions of the above, but for the transversion mutations
            (these should add up to 2 rather than 3, since each nucleotide has
            two possible transversions and one possible transition).
        """
        prefix = f"Pos {self.cp} for codon {self.codon}"

        if (self.si + self.ni) != 3:
            raise WeirdError(f"{prefix} has Si + Ni == {self.si + self.ni}?")

        if (self.si_tv + self.ni_tv) != 2:
            raise WeirdError(
                f"{prefix} has (Tv): Si + Ni == {self.si_tv + self.ni_tv}?"
            )

        if self.in_sense_codon:
            if (self.nnsi + self.nsi) != 3:
                raise WeirdError(
                    f"{prefix} has NNSi + NSi == {self.nnsi + self.nsi}?"
                )
            if (self.nnsi_tv + self.nsi_tv) != 2:
                raise WeirdError(
                    f"{prefix} has (Tv): NNSi + NSi == "
                    f"{self.nnsi_tv + self.nsi_tv}?"
                )
        else:
            if (
                self.nnsi is not None
                or self.nsi is not None
                or self.nnsi_tv is not None
                or self.nsi_tv is not None
            ):
                raise WeirdError(
                    "For stop codons, NNSi and NSi should be None."
                )


def is_transversion(nt1, nt2):
    """True if the mutation nt --> nt2 is a transversion, False otherwise.

    Parameters
    ----------
    nt1: str
        One of "A", "C", "G", "T".

    nt2: str
        One of "A", "C", "G", "T". Should be different from nt1.

    Raises
    ------
    WeirdError
        If nt1 == nt2.
        If nt1 or nt2 is not a nucleotide character.
        If something goes unexpectedly wrong.
    """
    if nt1 == nt2:
        raise WeirdError(f"is_transversion() called with nt1 == nt2 == {nt1}")

    # checking using "in nts" works as expected for non-string values; however,
    # checking using "in 'ACGT'" fails, because you get a TypeError about the
    # left operand needing to be a string. So let's do the list method in order
    # to throw a more informative error (and also to prevent some junk like
    # nt1 == "AC" from slipping through).
    nts = ["A", "C", "G", "T"]
    if nt1 not in nts or nt2 not in nts:
        raise WeirdError(
            "is_transversion() parameters are not both str nucleotides. "
            f"nt1 == {nt1}, nt2 == {nt2}. Check types?"
        )

    if nt1 == "A":
        return nt2 != "G"
    elif nt1 == "C":
        return nt2 != "T"
    elif nt1 == "G":
        return nt2 != "A"
    elif nt1 == "T":
        return nt2 != "C"
    else:
        raise WeirdError("Call a priest")


def get_poss_mutation_type_info():
    """Returns information about possible mutations for each CP of each codon.

    In theory, this information doesn't change unless you start considering
    non-standard genetic codes (or other weird things like alternative start
    codons). So we could just compute it once, store the results in config.py,
    and then not bother. But it's simpler to just compute it on the fly,
    anyway; plus this should make it easier to set up stuff like support for
    other genetic codes later on, on the off chance that people want that.

    Parameters
    ----------
    None

    Returns
    -------
    codon2cp2mts: dict
        Maps each of the 64 codons (formatted as strings, not skbio.DNA
        objects, for the sake of convenience) to another dict.

        Each of these "inner" dicts maps the ints 1, 2, and 3 (the three codon
        positions in this codon) to a CodonPositionMutationCounts object, which
        describes the number of possible Synonymous (si), Nonsynonymous (ni),
        Non-Nonsense (nnsi), and Nonsense (nsi) mutations at this codon
        position in this codon (for the standard genetic code). It also
        contains versions of these properties ending in _tv, indicating which
        of these types of mutations are also transversions.

    Raises
    ------
    WeirdError
        Will come up if something goes unexpectedly wrong during this process
        -- but this shouldn't happen in practice. (Maybe if the installation of
        scikit-bio is corrupted somehow?)

    Examples
    --------
    Consider CP 3 of the codon TGC. In the standard genetic code, here all of
    the four codons that start with "TG":

    TGA: Stop
    TGC: Cysteine
    TGG: Tryptophan
    TGT: Cysteine

    So, we should see:
    Si = 1 (one synonymous mutation, TGC --> TGT),
    Ni = 2 (two nonsynoymous mutations, TGC --> TGA and TGC --> TGG),
    NNSi = 2 (two non-nonsense mutations, TGC --> TGT and TGC --> TGG), and
    NSi = 1 (one nonsense mutation, TGC --> TGA).

    >>> codon2cp2mts = get_poss_mutation_type_info()
    >>> codon2cp2mts["TGC"][3].si
    1
    >>> codon2cp2mts["TGC"][3].ni
    2
    >>> codon2cp2mts["TGC"][3].nnsi
    2
    >>> codon2cp2mts["TGC"][3].nsi
    1
    >>> codon2cp2mts["TGC"][3].si_tv  # C --> T is a transition
    0
    >>> codon2cp2mts["TGC"][3].ni_tv  # C -> A and C --> T are both tvs
    2
    >>> codon2cp2mts["TGC"][3].nnsi_tv
    1
    >>> codon2cp2mts["TGC"][3].nsi_tv
    1

    Also, let's demonstrate that for stop codons NNSi and NSi should both be
    None:

    >>> codon2cp2mts["TGA"][3].si
    0
    >>> codon2cp2mts["TGA"][3].ni
    3
    >>> codon2cp2mts["TGA"][3].nnsi is None
    True
    >>> codon2cp2mts["TGA"][3].nsi is None
    True
    >>> codon2cp2mts["TGA"][3].si_tv
    0
    >>> codon2cp2mts["TGA"][3].ni_tv
    2
    >>> codon2cp2mts["TGA"][3].nnsi_tv is None
    True
    >>> codon2cp2mts["TGA"][3].nsi_tv is None
    True
    """
    codon2cp2mts = {}
    dna = "ACGT"
    for x in dna:
        for y in dna:
            for z in dna:
                codon = x + y + z
                codon2cp2mts[codon] = {
                    1: CodonPositionMutationCounts(codon, 1),
                    2: CodonPositionMutationCounts(codon, 2),
                    3: CodonPositionMutationCounts(codon, 3),
                }

    return codon2cp2mts


def compute_specific_mutation_decoy_contig_mut_rates(
    bcf_obj,
    thresh_type,
    thresh_vals,
    contig,
    pos_to_consider,
    mutation_types,
    contig_seq,
    contig_genes_df,
    codon2cp2mts,
):
    """Computes "specific" mutation rates for positions in a contig.

    This function should be applicable to (as of writing, at least) all decoy
    contexts besides "Full" and "CP2". Unlike these contexts (see
    compute_any_mutation_decoy_contig_mut_rates() for reference), we do not
    necessarily consider any mutation in the relevant positions in the contig
    towards the decoy -- we only consider specific types of mutations, for
    example just nonsynonymous mutations.

    If you'd like to filter based on positions, as well, then that can be done
    using pos_to_consider. NOTE that this function won't filter unreasonable
    positions for you, or positions in multiple genes -- you should adjust
    pos_to_consider to do that.

    Parameters
    ----------
    bcf_obj: pysam.VariantFile
        Object describing a BCF file produced by strainFlye's naive calling.

    thresh_type: str
        Either "p" or "r", depending on which type of mutations were called in
        bcf_obj.

    thresh_vals: range
        Range of values of p or r (depending on thresh_type) at which to
        compute mutation rates for this contig. Must use a step size of 1.
        If a mutation is a mutation for a value of p or r larger than the
        maximum p or r value here (i.e. it's "indisputable"), it will not be
        included in the mutation rate computation.

    contig: str
        Decoy contig name.

    pos_to_consider: set
        All (1-indexed) positions to consider in this contig. NOTE that, unlike
        compute_any_mutation_decoy_contig_mut_rates(), this must be a set --
        passing in None to consider all positions in the contig won't work.

        You should have already filtered out unreasonable positions in this
        list, as well as (for contexts involving Nonsyn/Nonsense/CP2) positions
        in multiple genes. And if your context involves CP2, then
        pos_to_consider should just be single-gene CP2 positions.

        UHHH hope that makes it clear lol

    mutation_types: list of str
        Types of mutation we will specifically count towards our decoy. Each
        entry should be one of "Nonsyn", "Nonsense", or "Tv". These are applied
        in an "AND filter" -- if you pass "Nonsyn" and "Tv", then we'll only
        count nonsynonymous transversion mutations towards our decoy (mutations
        that are just nonsynonymous but not transversions, or vice versa, won't
        be counted).

        Note that including both "Nonsyn" and "Nonsense" in this list will
        cause an error -- all nonsense mutations are nonsynonymous, so you can
        just say "Nonsense". Also, if this is list is empty, that will also
        cause an error.

    contig_seq: skbio.DNA
        Contig sequence. Unlike compute_any_mutation_decoy_contig_mut_rates(),
        here we will need to know the assembled "reference" sequence for this
        contig.

    contig_genes_df: pd.DataFrame or None
        Describes predicted genes in the contig sequence. There's a chance that
        this isn't necessary (if mutation_types only contains "Tv", then we
        won't care about genes) -- so, in that case, it's ok for this to be
        None.

    codon2cp2mts: dict or None
        The output of get_poss_mutation_type_info(), mapping codons to CPs to
        the numbers of possible (non)synonymous and (non)nonsense mutations.
        As with contig_genes_df, this won't be necessary if mutation_types
        only contains "Tv", so this can be None in that case.

    Returns
    -------
    mut_rates: list of float
        Mutation rates for each threshold value in thresh_vals.

    Raises
    ------
    ParameterError
        See get_mutation_types().

    Notes
    -----
    Sorry the parameters to this function are in such a bizarre order, and that
    there are so many of them. I can offer no excuse for this; the failure
    stems, ultimately, from whom I am as a person, and I can only hope that no
    other soul must be brought to madness by needing to understand this code.


    (that's a joke i'm just tired lmao)
    """
    min_val, high_val = extract_and_verify_thresh_vals(thresh_vals)
    num_muts = [0] * len(thresh_vals)

    tv, nonsyn, nonsense = get_mutation_types(mutation_types)

    # The number of *possible* mutations of each type, throughout the contig
    # sequence. This is the same across all threshold values.
    num_poss_muts = 0

    # If Nonsyn or Nonsense are in effect, we'll consider just positions
    # located in predicted genes. If this is *not* the case, then we've gotta
    # consider all positions -- this requires some special-casing, at least as
    # far as I can tell.
    if tv and (not nonsyn) and (not nonsense):
        # Covers contexts "Tv", "CP2Tv"

        # Regardless of the reference nucleotide (A/C/G/T), there are exactly
        # two transversion mutations possible at every position.
        num_poss_muts = len(pos_to_consider) * 2

        for mut in bcf_obj.fetch(contig):
            if mut.pos in pos_to_consider:
                if is_transversion(mut.ref.upper(), mut.alts[0].upper()):
                    # this is a transversion mutation
                    update_number_of_mutations_at_thresholds(
                        num_muts, thresh_type, min_val, high_val, mut
                    )
    else:
        # Covers contexts "Nonsyn", "Nonsense", "CP2Nonsyn", "CP2Nonsense",
        # "TvNonsyn", "TvNonsense", "CP2TvNonsyn", "CP2TvNonsense"

        # Figure out all mutated positions in this contig and their ref/alt nts
        mp2ra = bcf_utils.get_mutated_position_details_in_contig(
            bcf_obj, contig, zero_indexed=False
        )

        # Set of (0-indexed) mutated positions in the contig that pass our
        # checks. We build up this set and then update the number of observed
        # mutations, rather than updating the number of observed mutations
        # gradually, because we need to have access to the mutation records in
        # the BCF in order to call update_number_of_mutations_at_thresholds().
        # (I guess I could write a new utility function that loads all records
        # at once into memory, but that might be too inefficient. But whatever,
        # it doesn't matter too much, this only happens for one contig, I'm
        # going to stop writing this comment now)
        passing_mutated_positions = set()

        # We know at this point that either Nonsyn or Nonsense is given: so we
        # will implicitly limit ourselves to considering positions in genes.
        #
        # (Note that multi-gene positions should've already been removed from
        # pos_to_consider before this function, so we don't bother checking for
        # those here).
        for gene in contig_genes_df.itertuples():

            # Adjust the order with which we iterate through the gene's
            # positions: it's simpler, I think, to go from Left --> Right
            # for "+" Strand genes and from Left <-- Right for "-" Strand
            # genes. This way, the current codon position always goes 123123...
            # Note that "gene_positions" includes 1-indexed positions and is
            # inclusive on both ends
            if gene.Strand == "+":
                gene_positions = range(gene.LeftEnd, gene.RightEnd + 1)
            else:
                gene_positions = range(gene.RightEnd, gene.LeftEnd - 1, -1)

            # Should never happen (tm) (c) ... but let's catch it if it does.
            if len(gene_positions) % 3 == 0:
                raise WeirdError(
                    f"Gene {gene.Index} has length {len(gene_positions):,}?"
                )

            cp = 1
            curr_codon_cp1_pos = None
            for pos in gene_positions:
                if cp == 1:
                    curr_codon_cp1_pos = pos

                # Ignore unreasonable or multi-gene positions (oh, or also any
                # other filtering that was done before this function was called
                # -- e.g. for the "CP2" contexts, pos_to_consider will only
                # include CP2 positions)
                if pos in pos_to_consider:

                    # Get parent codon from the contig sequence. Note that the
                    # contig_seq object (of type skbio.DNA) is 0-indexed, so
                    # we've gotta take that into account here.
                    if gene.Strand == "+":
                        parent_codon = str(
                            contig_seq[
                                curr_codon_cp1_pos - 1 : curr_codon_cp1_pos + 2
                            ]
                        )
                    else:
                        parent_codon = str(
                            contig_seq[
                                curr_codon_cp1_pos - 3 : curr_codon_cp1_pos
                            ]
                        )

                    # Now that we know the parent codon and the current CP we
                    # are on, figure out how many possible mutations (of the
                    # type(s) we care about) are possible at this position.
                    # We already know this information thanks to codon2cp2mts.

                    cpm = 0
                    if nonsyn:
                        if tv:
                            cpm = codon2cp2mts[parent_codon][cp].ni_tv
                        else:
                            cpm = codon2cp2mts[parent_codon][cp].ni
                    elif nonsense:
                        if tv:
                            cpm = codon2cp2mts[parent_codon][cp].nsi_tv
                        else:
                            cpm = codon2cp2mts[parent_codon][cp].nsi
                    else:
                        raise WeirdError(
                            "At this point in the code, either Nonsyn or "
                            "Nonsense should be True."
                        )

                    # If no mutations of the type we care about are possible at
                    # this position, move on.
                    if cpm == 0:
                        continue

                    # Otherwise, we're in business.
                    num_poss_muts += cpm

                    # Is this position actually a mutation? (Given the minimum
                    # threshold value that call used -- if so, then we still
                    # gotta see how this varies as we adjust the threshold.)
                    if pos in mp2ra:
                        ref_nt, alt_nt = mp2ra[pos]

                        # Would mutating from ref_nt to alt_nt be the sort of
                        # mutation we care about (tv? nonsyn? nonsense?)
                        if tv and not is_transversion(ref_nt, alt_nt):
                            continue
                        # OK, so this mutation "passes" the transversion check
                        # (or lack thereof)

                        # Next, let's check the nonsyn/nonsense stuff
                        is_si, is_nnsi = get_mutation_types_for_cp(
                            parent_codon, cp, alt_nt
                        )
                        if nonsyn and is_si:
                            continue

                        if nonsense and is_nnsi:
                            continue

                        # OK, if we've made it here we've passed everything.
                        # The mutation at this position is the type we care
                        # about.
                        passing_mutated_positions.add(pos - 1)

                # We already know that gene_positions respects the gene's
                # strand, so we can safely update the CP by going 123123...
                cp = get_next_cp(cp, gene)

        # Okay, now that we've seen all mutated positions passing our checks,
        # let's update the number of *observed* mutations at each of the
        # threshold values we are considering.
        for mut in bcf_obj.fetch(contig):
            if mut.pos in passing_mutated_positions:
                update_number_of_mutations_at_thresholds(
                    num_muts, thresh_type, min_val, high_val, mut
                )

    return [n / num_poss_muts for n in num_muts]


def get_next_cp(cp, gene, ascending=True):
    """Updates a 1-indexed codon position.

    Intended to be used while iterating through the positions in a
    (protein-coding) gene. There's probably a more elegant way to do this, but
    this seems like the clearest (and least prone to off by one errors) way.

    ...BREAKING NEWS: local grad student "too stupid to use basic modulo
    arithmetic"; bystanders SHOCKED at this SUSSY IMPOSTOR BEHAVIOR; more at 11

    Parameters
    ----------
    cp: int
        Either 1, 2, or 3. (Hopefully.)

    gene: namedtuple with Index and Strand properties
        Row in a DataFrame (produced by parse_sco()) representing a gene.
        Like, if you say something like "for gene in df.itertuples():",
        then you can just pass "gene" directly to this function.

        The main purpose of this is to allow us to throw helpful error
        messages, so we can see *where* iteration broke (rather than just "hey
        something went wrong at some point").

    ascending: bool
        If True,  go from 1 --> 2 -- > 3 --> 1 --> ...;
        if False, go from 3 --> 2 -- > 1 --> 3 --> ...

    Returns
    -------
    next_cp: int

    Raises
    ------
    WeirdError
        Raised by complain_about_cps() if cp isn't 1, 2, or 3.
    """
    if ascending:
        if cp == 1 or cp == 2:
            return cp + 1
        elif cp == 3:
            return 1
        else:
            complain_about_cps(gene.Index, gene.Strand, cp)
    else:
        if cp == 3 or cp == 2:
            return cp - 1
        elif cp == 1:
            return 3
        else:
            complain_about_cps(gene.Index, gene.Strand, cp)


def complain_about_cps(gn, gs, cp):
    """Raises an error about codon position while iterating through a gene.

    Mostly intended as a fail-safe to catch weird errors. See
    get_next_cp() for context on why this function is even useful.

    Parameters
    ----------
    gn: int
        ID number for a gene.

    gs: str
        Gene strand. Should be "+" or "-", but we don't do any sanity checking
        here.

    cp: int
        Codon position. In practice, this function should be called if this
        gets "off" somehow (i.e. it isn't 1, 2, or 3).

    Raises
    ------
    WeirdError
    """
    raise WeirdError(
        f"Codon position got out of whack: gene {gn:,}, strand {gs}, CP {cp}"
    )


def get_single_gene_and_cp2_positions(genes_df, fail_if_no_sgcp2=True):
    """Returns positions located in exactly one gene and, also, that plus CP2.

    Parameters
    ----------
    genes_df: pd.DataFrame
        Describes predicted genes in a contig. See parse_sco().

    fail_if_no_cp2: bool
        Determines how we handle the rare case when there is at least one
        single-gene position, but no single-gene CP2 positions. If True, raise
        an error; if False, just return an empty set for single-gene CP2
        positions.

    Returns
    -------
    single_gene_pos, single_gene_cp2_pos: set
        single_gene_pos describes all positions located in exactly one gene.

        single_gene_cp2_pos is the intersection of single_gene_pos and all
        positions that are in CP2 (the second codon position) of their single
        parent gene.

        Both of these sets of positions use 1-indexing.

    Raises
    ------
    WeirdError
        If something goes wrong during iteration; this should never happen, but
        you never know.

    ParameterError
        If no single-gene positions exist. (In the rare case where there exist
        some single-gene positions but no single-gene CP2 positions, whether or
        not we will fail or just return an empty set for single_gene_cp2_pos
        depends on the fail_if_no_cp2 parameter.)

        This situation could actually happen, I guess, if the genes predicted
        overlap almost completely.

    Notes
    -----
    This is pretty inefficient. I doubt it will be a bottleneck, since this
    should only be called once per run of "fdr estimate", but if you need to
    speed this up a good first step might be moving from itertuples() to
    something like .apply() / vectorization.
    """
    # records all positions located in at least one gene
    genic_pos = set()
    # records all positions in at least one gene that we have seen thus far
    pos_seen_in_other_genes = set()
    # records all positions known to be in multiple genes
    multi_gene_pos = set()
    # records all positions in CP2 of at least one gene
    cp2_pos = set()

    # PERF: yeah yeah yeah use something faster than itertuples
    for gene in genes_df.itertuples():
        if gene.Strand == "+":
            cp = 1
        else:
            cp = 3
        for pos in range(gene.LeftEnd, gene.RightEnd + 1):

            genic_pos.add(pos)

            # if we've already seen this position in another gene, then this
            # position is contained in multiple genes. So: we won't include it
            # as part of the decoy.
            if pos in pos_seen_in_other_genes:
                multi_gene_pos.add(pos)

            # Keep track of ALL CP2 positions we see, even those in multiple
            # genes. (We could try to only do this if the above check is False,
            # but that wouldn't prevent against the case where we see a
            # position and then later find out it's in another gene. Simpler to
            # do things this way; not gonna optimize this too much right now.)
            if cp == 2:
                cp2_pos.add(pos)

            # Make it clear that we've seen this position in at least one gene.
            pos_seen_in_other_genes.add(pos)

            # Update the current codon position.
            # I guess it's worth noting that we don't really *need* to care
            # about whether this goes 123123... or 321321... (determined by the
            # "ascending" parameter of get_next_cp(), since (for an arbitrary
            # gene) the CP2 positions will be the same regardless of strand.
            # But, oh well, this is how i initially wrote the code; no sense
            # intentionally making it more confusing.
            cp = get_next_cp(cp, gene, ascending=(gene.Strand == "+"))

    single_gene_pos = genic_pos - multi_gene_pos
    single_gene_cp2_pos = single_gene_pos & cp2_pos

    if len(single_gene_pos) == 0:
        raise ParameterError(
            "No single-gene positions exist (given the predicted genes)."
        )

    if fail_if_no_sgcp2 and len(single_gene_cp2_pos) == 0:
        raise ParameterError(
            "No single-gene CP2 positions exist (given the predicted genes)."
        )

    return single_gene_pos, single_gene_cp2_pos


def compute_target_contig_fdr_curve_info(
    bcf_obj,
    thresh_type,
    thresh_vals,
    target_contig,
    target_contig_len,
    ctx2mr,
):
    """Computes FDR curve information for a given target contig.

    The intent is to create output that can be dropped straight into a TSV
    file, without too much work on the part of the caller.

    Parameters
    ----------
    bcf_obj: pysam.VariantFile
        Object describing a BCF file produced by strainFlye's naive calling.

    thresh_type: str
        Either "p" or "r", depending on which type of mutations were called in
        bcf_obj.

    thresh_vals: range
        Range of values of p or r (depending on thresh_type) at which to
        count mutations in this contig. Must use a step size of 1.
        If a mutation is a mutation for a value of p or r larger than the
        maximum p or r value here (i.e. it's "indisputable"), it will not be
        included in the FDR curve information computation.

    target_contig: str
        Name of the target contig.

    target_contig_len: int
        Target contig sequence length.

    ctx2mr: dict
        Maps values in decoy_contexts to a list of mutation rates computed for
        the decoy contig using this context. See
        compute_decoy_contig_mut_rates() for details.

    Returns
    -------
    (ctx2fdr_lines, num_line): (dict, str)
        Each value in ctx2fdr_lines (and num_line itself) are tab-separated
        lines (suitable for adding to a TSV file where the first column is the
        target contig name and there are len(thresh_vals) additional columns).

        The first entry in each line in ctx2fdr_lines, and the first entry in
        num_line, is the target contig name. The remaining entries in each
        line in ctx2fdr_lines describe the estimated FDR for this target contig
        for the decoy mutation rate at this context at each threshold value
        (aka the x-axis on a FDR curve, as drawn in the paper). The
        remaining entries in num_line describe the number of mutations per
        megabase for this target contig at each threshold value (the y-axis
        on a FDR curve, as drawn in the paper).
    """
    # Get the number of mutations in this target contig at each of the
    # threshold values
    num_muts = compute_number_of_mutations_in_contig(
        bcf_obj, thresh_type, thresh_vals, target_contig
    )

    # it's a long story. see docs for compute_num_mutations_per_mb() in
    # https://github.com/fedarko/sheepgut/blob/main/notebooks/DemonstratingTargetDecoyApproach.ipynb
    numpermb_coeff = 1000000 / target_contig_len
    denominator = 3 * target_contig_len
    # The 100 is because we convert FDRs from [0, 1] to percentages
    # (technically, FDRs can exceed 100% if the decoy mutation rate > the
    # target mutation rate, but that shouldn't happen too often)
    fdr_coeff = 100 * denominator

    ctx2fdr_lines = {ctx: target_contig for ctx in ctx2mr}
    num_line = target_contig

    # TODO: Maybe speed this up by first creating a pandas DataFrame of all
    # contigs and their num_muts, then using vectorization or apply() to do
    # this stuff on all contigs at once? But this runs in ~2 minutes on the
    # entire SheepGut dataset (edit: or at least it did before i added
    # context-dependent stuff back in), so it's not a substantial bottleneck.

    for i, n in enumerate(num_muts):
        if n == 0:
            # The FDR is undefined if the target's mutation rate is zero
            for ctx in ctx2mr:
                ctx2fdr_lines[ctx] += "\tNA"
            # And, of course, there are 0 mutations / Mb
            num_line += "\t0"

        else:
            # the (fdr coeff / n) thing is the same for all contexts at this
            # threshold, so we can pre-compute it and then reuse it for all the
            # different contexts for this target contig for this threshold
            # value. might be overkill but whatevs.
            fdr_cell_coeff = fdr_coeff / n
            for ctx in ctx2mr:
                ctx2fdr_lines[ctx] += "\t" + str(
                    fdr_cell_coeff * ctx2mr[ctx][i]
                )

            num_line += f"\t{numpermb_coeff * n}"

    # "cross your t's, dot your i's, and add your newlines" -- hillary clinton
    for ctx in ctx2fdr_lines:
        ctx2fdr_lines[ctx] += "\n"
    num_line += "\n"

    return ctx2fdr_lines, num_line


def parse_sco(sco_fp):
    """Returns a DataFrame representing a SCO file describing gene predictions.

    This function is mostly intended for internal use at the moment, since I
    expect that -- in the case of e.g. the hotspot features stuff -- most
    people will have GFF3 files. (But if a lot of people have SCO files, then
    I guess we could modify that side of things to accept these as well.)

    Parameters
    ----------
    sco_fp: str
        Filepath to a SCO ("Simple Coordinate Output") file.

    Returns
    -------
    df: pd.DataFrame
        Describes genes in the SCO file. Rows are indexed based on the
        1-indexed gene number in the SCO file; there are four columns,
        "LeftEnd", "RightEnd", "Length", and "Strand". LeftEnd and RightEnd
        are 1-indexed and inclusive.

    Raises
    ------
    WeirdError
        If various things look wrong with the SCO file. I raise this instead
        of a ParameterError, because problems here indicate that (probably)
        something is up with Prodigal, rather than with how the user is running
        strainFlye. (Although I guess we could have multiple sources of
        error...)

    ValueError
        Implicitly raised if things we expect to be integers in the SCO file
        (e.g. gene coordinates) are not.

    References
    ----------

    +---------------------------------+
    | What does a SCO file look like? |
    +---------------------------------+

    Prodigal's documentation
    (https://github.com/hyattpd/Prodigal/wiki/understanding-the-prodigal-output)
    points to http://tico.gobics.de/ioexamples.jsp regarding details of the SCO
    file format. As of writing (September 4, 2022), this link is dead.
    But! We can figure out what it said with the wayback machine:
    https://web.archive.org/web/20060717224112/http://tico.gobics.de/ioexamples.jsp

    Copying from there:

        The Simple Coord format just gives the coordinates of the predicted
        ORFs with an id and the orientation. The input may also contain a
        score and a label as in the Simple Coord output of TiCo.

        >id_left_right_strand[_score][#]

        Example:

        >2_337_2799_+
        >3_2801_3733_+
        >5_3734_5020_+
        >6_5088_5237_+
        >8_5310_5720_-
        >10_5683_6459_-
        >12_6529_7959_-
        >14_8175_9191_+
        >15_9303_9893_+
        >17_9928_10494_-
        >19_10643_11356_-

    Here, we ignore the presence of a score / label, and just focus on
    processing the first four fields.

    +--------------------------------------+
    | ok but like why does this code exist |
    +--------------------------------------+

    I initially wrote this to mimic the way LST files from GeneMark
    can be parsed with Pandas (...with some effort). Hence the reuse of
    "LeftEnd", "RightEnd", etc. I figured it was simpler to adapt this code
    then it was to re-write stuff to work with GFF3 files.

    Also, the initial version of this code comes from the SheepGut repo:
    https://github.com/fedarko/sheepgut/blob/main/notebooks/parse_sco.py
    """
    genes = {}
    with open(sco_fp, "r") as f:
        for line in f:
            # Ignore comments
            if not line.startswith("#"):
                if line.startswith(">"):
                    # Ignore any "parts" after the strand -- we don't care
                    # about scores/labels in SCO files (see refs above)
                    parts = line[1:].strip().split("_")

                    # ValueErrors will be raised if any of these parts don't
                    # look like a number
                    gene_num = int(parts[0])
                    left_end = int(parts[1])
                    rght_end = int(parts[2])

                    strand = parts[3]
                    if strand != "+" and strand != "-":
                        raise WeirdError(
                            f"Unrecognized strand in SCO: {strand}"
                        )

                    # left end should always be on the left side, regardless of
                    # strand
                    if left_end >= rght_end:
                        raise WeirdError(
                            f"Gene {gene_num:,} in SCO has left end of "
                            f"{left_end:,}, which is not < the right end of "
                            f"{rght_end:,}."
                        )

                    length = rght_end - left_end + 1
                    if length % 3 != 0:
                        raise WeirdError(
                            f"Gene {gene_num:,} in SCO is {length:,} bp long; "
                            "lengths must be divisible by 3."
                        )

                    genes[gene_num] = [left_end, rght_end, length, strand]
                else:
                    # If this line doesn't start with # or > and it isn't just
                    # a blank line, then something's up. Throw an error.
                    stripped_line = line.strip()
                    if len(stripped_line) > 0:
                        raise WeirdError(
                            "Unrecognized line prefix in SCO: line = "
                            f'"{stripped_line}"'
                        )

    # shouldn't happen unless something really weird happens with the decoy
    # genome, but let's account for it anyway
    if len(genes) < 1:
        raise WeirdError("No genes described in SCO.")

    df = pd.DataFrame.from_dict(
        genes,
        orient="index",
        columns=["LeftEnd", "RightEnd", "Length", "Strand"],
    )
    return df


def get_prodigal_genes(seq, name):
    """Runs Prodigal on a sequence and returns info about the predicted genes.

    Parameters
    ----------
    seq: skbio.DNA
        Sequence in which we'll predict genes.

    name: str
        Name of this sequence. Just used for naming the temporary files; this
        shouldn't matter, but it could be useful for debugging.

    Returns
    -------
    df: pd.DataFrame
        Describes genes predicted in this sequence. See parse_sco()'s docs for
        details on this. (Importantly, the coordinates are 1-indexed and
        inclusive.)

    Raises
    ------
    subprocess.CalledProcessError
        If Prodigal has a problem. I imagine the most likely cause would be
        really short sequences; Prodigal will throw an error if the input
        sequence has < 20,000 characters, at least as of writing.

    Notes
    -----
    This assumes that the sequence is prokaryotic, since Prodigal is only
    designed for bacterial and archaeal genomes; see
    https://github.com/hyattpd/Prodigal/wiki/introduction.
    """
    with tempfile.TemporaryDirectory() as td:
        # Write out the sequence to a file
        # (yeah we could pipe it into prodigal but i don't trust myself to
        # do that correctly)
        fasta_fp = os.path.join(td, f"{name}.fasta")
        with open(fasta_fp, "w") as dffh:
            skbio.io.write(seq, format="fasta", into=dffh)

        sco_fp = os.path.join(td, f"{name}.sco")

        subprocess.run(
            ["prodigal", "-i", fasta_fp, "-o", sco_fp, "-f", "sco", "-c"],
            check=True,
        )

        # Read in and return the predicted genes
        return parse_sco(sco_fp)


def compute_decoy_contig_mut_rates(
    contigs,
    bam_obj,
    bcf_obj,
    thresh_type,
    thresh_vals,
    decoy_contig,
    decoy_contexts,
    fancylog,
):
    """Computes mutation rates for a decoy contig at some threshold values.

    Parameters
    ----------
    contigs: str
        Filepath to a FASTA file containing contigs in which mutations were
        naively called. We'll only really use this to extract the decoy
        contig's sequence (it's probably easier to load it here then to rely on
        the caller to load it).

    bam_obj: pysam.AlignmentFile
        Object describing a BAM file mapping reads to contigs. Used to classify
        positions as "unreasonable" or not (because we need to be able to do
        this for *all* positions in the decoy contig, not just mutated
        positions) (seriously, what kind of chump would spend an hour encoding
        that info in the BCF before realizing it was the wrong way to solve
        this problem) (i bet his name is "marcus" or something stupid) (god i
        hate that guy)

    bcf_obj: pysam.VariantFile
        Object describing a BCF file produced by strainFlye's naive calling.

    thresh_type: str
        Either "p" or "r", depending on which type of mutations were called in
        bcf_obj.

    thresh_vals: range
        Range of values of p or r (depending on thresh_type) at which to
        compute mutation rates. Must use a step size of 1.

    decoy_contig: str
        Name of a contig to compute mutation rates for.

    decoy_contexts: list of str
        Context-dependent mutation settings to apply in computing the mutation
        rates. Each entry should be contained in config.DECOY_CONTEXTS.
        Details on these contexts' meanings:

         - "Full": Compute mutation rates across the entire contig.

         - "CP2": Only consider positions that are 1) located in a single
                  predicted protein-coding gene and 2) are located in the
                  second codon position of this gene.

         - "Tv": Compute mutation rates based on treating potential
                 transversion mutations (A <-> C, G <-> T, A <-> T, C <-> G)
                 as a decoy.

         - "Nonsyn": Compute mutation rates based on treating potential
                     nonsynonymous mutations (for positions located in a single
                     predicted protein-coding gene) as a decoy.

         - "Nonsense": Like Nonsyn, but for nonsense mutations.

         - "CP2Tv": Tv, but only for positions in CP2.
         - "CP2Nonsyn": Nonsyn, but only for positions in CP2.
         - "CP2Nonsense": Nonsense, but only for positions in CP2.
         - "TvNonsyn": Tv + Nonsyn mutations.
         - "TvNonsense": Tv + Nonsense mutations.
         - "CP2TvNonsense": Tv + Nonsense, but only for positions in CP2.
         - "CP2TvNonsyn": Tv + Nonsyn, but only for positions in CP2.

        ... Note that we don't have any options for combining Nonsyn and
        Nonsense, because this wouldn't do anything (all nonsense mutations are
        by definition nonsynonymous). Arguably, CP2 + Nonsyn is also *almost*
        useless, but there is one possible "synonymous" CP2 mutation (TGA <->
        TAA both code for a stop codon) so we include that.

    fancylog: function
        Logging function.

    Returns
    -------
    ctx2mr: dict
        Maps each entry in decoy_contexts to a list of mutation rates computed
        for the decoy contig using this context.

        Each list has the same dimensions as thresh_vals. So, the t-th
        entry in a list corresponds to the mutation rate computed for the
        decoy contig (using the current decoy context) based on naive calling
        using the t-th threshold value.

    Raises
    ------
    SequencingDataError
        If decoy_contig isn't in the contigs. (This should never happen if
        this function is called from run_estimate(), which should already have
        ensured that this is the case.)

    subprocess.CalledProcessError
        Raised if something goes wrong with Prodigal (e.g. the decoy contig is
        < 20,000 bp long). See get_prodigal_genes().

    References
    ----------
    See https://www.mun.ca/biology/scarr/Transitions_vs_Transversions.html for
    details on transversions.
    """
    # NOTE: technically we don't need to load the decoy sequence for all of the
    # decoy contexts, but let's do it anyway -- has the nice side effect of
    # triggering an error if the decoy contig isn't in the FASTA.
    decoy_seq = fasta_utils.get_single_seq(contigs, decoy_contig)

    # Figure out if we gotta run Prodigal on the decoy contig (yes if any of
    # the contexts include CP2, Nonsyn, or Nonsense)
    decoy_genes_df = None
    has_genic_decoy_context = False
    has_cp2_decoy_context = False
    decoy_single_gene_pos = None
    decoy_single_gene_cp2_pos = None

    # Figure out if we need to determine unreasonable positions (yes if any of
    # the contexts include Nonsyn, Nonsense, or Tv)
    unreasonable_positions = None
    has_specific_mutation_decoy_context = False

    # Figure out if we need to compute Codon --> CP --> Possible Mutation Type
    # info (yes if any of the contexts include Nonsyn or Nonsense)
    codon2cp2mts = None
    has_nn_decoy_context = False

    for ctx in decoy_contexts:
        if "CP2" in ctx:
            has_genic_decoy_context = True
            has_cp2_decoy_context = True

        elif "Nonsyn" in ctx or "Nonsense" in ctx:
            has_genic_decoy_context = True
            has_specific_mutation_decoy_context = True
            has_nn_decoy_context = True

        elif "Tv" in ctx:
            has_specific_mutation_decoy_context = True

    # Ok we gotta run Prodigal
    if has_genic_decoy_context:
        fancylog(
            (
                "At least one of the decoy context(s) requires us to have "
                f"gene predictions for {decoy_contig}; running Prodigal..."
            ),
            prefix="",
        )
        # will raise a SequencingDataError if decoy_contig isn't in this FASTA
        decoy_genes_df = get_prodigal_genes(decoy_seq, decoy_contig)
        fancylog(
            (
                "Finished running Prodigal! It predicted "
                f"{len(decoy_genes_df.index):,} genes in {decoy_contig}."
            ),
            prefix="",
        )

        # compute single-gene CP2 positions even if "CP2" isn't in any of the
        # contexts; we're iterating through the genes anyway, so this doesn't
        # require much extra time
        (
            decoy_single_gene_pos,
            decoy_single_gene_cp2_pos,
        ) = get_single_gene_and_cp2_positions(
            decoy_genes_df, fail_if_no_sgcp2=has_cp2_decoy_context
        )

        # compute "possible" mutation type info, if needed
        if has_nn_decoy_context:
            codon2cp2mts = get_poss_mutation_type_info()

    # Get a set of "unreasonable" positions (1-indexed):
    # these are positions where the consensus and reference don't match.
    if has_specific_mutation_decoy_context:
        fancylog(
            (
                "At least one of the decoy context(s) requires us to know "
                '"unreasonable" positions (where reference and consensus '
                "disagree) in {decoy_contig}; going through alignment..."
            ),
            prefix="",
        )
        unreasonable_positions = set()
        for pos, rec in enumerate(
            pysamstats.stat_variation(
                bam_obj,
                chrom=decoy_contig,
                fafile=contigs,
                pad=True,
                max_depth=config.MAX_DEPTH_PYSAM,
            ),
            1,
        ):
            max_nt_freq = max([rec[nt] for nt in "ACGT"])
            if rec[str(decoy_seq[pos - 1])] < max_nt_freq:
                unreasonable_positions.add(pos)
        fancylog(
            (
                f"Done: identified {len(unreasonable_positions):,} "
                f"unreasonable positions in {decoy_contig}."
            ),
            prefix="",
        )

    # this'll be the main output
    ctx2mr = {}

    # save some repeated typing...
    params = [bcf_obj, thresh_type, thresh_vals, decoy_contig]

    num_ctxs = len(decoy_contexts)
    for ci, ctx in enumerate(decoy_contexts, 1):
        fancylog(
            (
                f'Computing mutation rates for decoy context "ctx" ({ci:,} / '
                f"{num_ctxs:,})..."
            ),
            prefix="",
        )

        if ctx == "Full":
            ctx2mr[ctx] = compute_any_mutation_decoy_contig_mut_rates(*params)

        elif ctx == "CP2":
            ctx2mr[ctx] = compute_any_mutation_decoy_contig_mut_rates(
                *params,
                pos_to_consider=decoy_single_gene_cp2_pos,
            )

        else:
            # for most contexts at this point, we'll just consider positions
            # located in a single gene (whether that's single-gene CP2
            # positions, or just single-gene positions). However, there is one
            # exception where we will look outside of genes -- if the context
            # is ONLY "Tv", then there's nothing binding us to looking at
            # positions in genes.
            if "CP2" in ctx:
                pos_to_consider = decoy_single_gene_cp2_pos
            elif ctx == "Tv":
                pos_to_consider = set(range(1, len(decoy_seq) + 1))
            else:
                pos_to_consider = decoy_single_gene_pos

            # In any case, we will ignore unreasonable positions
            pos_to_consider -= unreasonable_positions

            mutation_types = [
                mt for mt in ["Nonsyn", "Nonsense", "Tv"] if mt in ctx
            ]

            ctx2mr[ctx] = compute_specific_mutation_decoy_contig_mut_rates(
                *params,
                pos_to_consider,
                mutation_types,
                decoy_seq,
                decoy_genes_df,
                codon2cp2mts,
            )
        fancylog("Done computing those mutation rates.", prefix="")

    return ctx2mr


def run_estimate(
    contigs,
    bam,
    bcf,
    diversity_indices,
    decoy_contig,
    decoy_contexts,
    high_p,
    high_r,
    decoy_min_length,
    decoy_min_average_coverage,
    output_dir,
    fancylog,
    chunk_size=500,
):
    """Performs decoy selection and FDR estimation.

    Notably, both high_p and high_r will be defined regardless of if the BCF
    was generated using p- or r-mutations, because (unlike strainFlye call)
    I don't think it's worth splitting this step up into two sub-commands
    by p- or r-mutations. So we'll just ignore one of these values.

    Parameters
    ----------
    contigs: str
        Filepath to a FASTA file containing contigs in which mutations were
        naively called.

    bam: str
        Filepath to a BAM file mapping reads to contigs.
        (We use this here to determine whether or not a position is
        "unreasonable." We don't always need to do this -- for example, if the
        user only wants to use a "Full" and/or "CP2" decoy genome -- but it's
        easiest to just have the user pass this anyway, since if the user ran
        "call" then they should already have this around.)

    bcf: str
        Filepath to a BCF file generated by one of strainFlye call's
        subcommands.

    diversity_indices: str or None
        If a str, this should be a filepath to a TSV file containing diversity
        index info, also generated by one of strainFlye call's subcommands.
        We'll use this information to automatically select a decoy contig.

    decoy_contig: str or None
        If a str, this should be the name of a contig described in the
        BCF file. We'll use this as a decoy contig.

    decoy_contexts: list of str
        Context-dependent mutation settings (e.g. Full, CP2, Nonsyn, ...)
        Thankfully, we know each entry in this list will be one of a set of
        allowed choices, since we use click.Choice() to screen it at the CLI.
        We'll generate one TSV file for each of these contexts.

    high_p: int
        "Indisputable" threshold for p-mutations (scaled up by 100).

    high_r: int
        "Indisputable" threshold for r-mutations.

    decoy_min_length: int
        If automatically selecting decoy contigs, we'll only consider contigs
        that are at least this long.

    decoy_min_average_coverage: float
        If automatically selecting decoy contigs, we'll only consider contigs
        with average coverages of at least this.

    output_dir: str

    fancylog: function
        Logging function.

    chunk_size: int
        After seeing this many target contigs, we'll write out their FDR and (#
        mutations per Mb) information to the corresponding files.

    Returns
    -------
    None

    Raises
    ------
    ParameterError
        - If the selected decoy contig isn't present in the contigs file.
        - If the high p (or high r) threshold is <= the minimum threshold value
          used in the BCF file.

    We also call misc_utils.load_triplet(), which can cause a pretty large
    amount of errors to be raised if the input data looks weird -- see that
    function's docs for details.
    """
    # Check and load the FASTA, BAM, and BCF
    (
        contig_name2len,
        bam_obj,
        bcf_obj,
        thresh_type,
        thresh_min,
    ) = misc_utils.load_triplet(
        contigs,
        bam,
        bcf,
        fancylog,
        bcf_exact=True,
        min_num_contigs=2,
        get_sf_bcf_details=True,
    )
    # We *could* try to ensure that the diversity index file's contigs, if
    # a diversity index file is specified, are a subset of the BCF -- but
    # no need to do this extra work right now. the main thing that matters IMO
    # is just checking that the selected decoy contig is in the BCF, which is
    # much easier to do later.

    # Identify decoy contig
    selection_type = check_decoy_selection(diversity_indices, decoy_contig)
    if selection_type == "DI":
        fancylog("Selecting a decoy contig based on the diversity indices...")
        used_decoy_contig = autoselect_decoy(
            diversity_indices,
            decoy_min_length,
            decoy_min_average_coverage,
            fancylog,
        )
        fancylog(
            f"Selected {used_decoy_contig} as the decoy contig.", prefix=""
        )
    else:
        used_decoy_contig = decoy_contig
        fancylog(f"The specified decoy contig is {used_decoy_contig}.")

    # verify that the decoy contig is actually contained in the FASTA file
    if used_decoy_contig not in contig_name2len:
        raise ParameterError(
            f"Selected decoy contig {used_decoy_contig} is not present in "
            f"{contigs}."
        )
    fancylog(
        (
            "Verified that this decoy contig is present in the FASTA, BAM, "
            "and BCF files."
        ),
        prefix="",
    )

    # if someone chuckles at this, the project is successful
    # that's how it works
    fancylog("(Sorry for doubting you.)", prefix="")

    # Figure out the exact p or r values we'll iterate through -- these will
    # correspond to the columns in our output TSV of FDR estimation (i.e.
    # for each target contig, we'll produce this many FDR estimates).
    #
    # NOTE 1: For the time being, we just go through in increments of 1 (for
    # p, this is 0.01%; for r, this is just 1 read). We could support other
    # "step" values, but that's a lot of work for probably little benefit
    # (unless this ends up being a bottleneck, idk).
    #
    # NOTE 2: We do not produce an estimate for the exact high_p (or high_r)
    # value. The maximum threshold is that minus the step value (... which is
    # always 1, at least right now).
    fancylog(f"Determining range of value(s) of {thresh_type} to consider...")
    if thresh_type == "p":
        if high_p <= thresh_min:
            raise ParameterError(
                f"--high-p = {high_p:,} must be larger than the minimum p "
                f"used in the BCF ({thresh_min:,})."
            )
        thresh_high = high_p
    else:
        if high_r <= thresh_min:
            raise ParameterError(
                f"--high-r = {high_r:,} must be larger than the minimum r "
                f"used in the BCF ({thresh_min:,})."
            )
        thresh_high = high_r
    thresh_max = thresh_high - 1
    thresh_vals = range(thresh_min, thresh_high)
    fancylog(
        (
            f"We'll consider {len(thresh_vals):,} value(s) of {thresh_type}: "
            f"from {thresh_min:,} to {thresh_max:,}."
        ),
        prefix="",
    )

    # Extra clarification about indisputable mutations. I want to avoid taking
    # people by surprise with this.
    fancylog(
        (
            f"{thresh_type}-mutations for {thresh_type} \u2265 "
            f'{thresh_high:,} will be considered "indisputable."'
        ),
        prefix="",
    )
    fancylog(
        (
            'These "indisputable" mutations won\'t be included in the FDR '
            "estimation results."
        ),
        prefix="",
    )

    # Click allows the user to specify the same Choice multiple times, which
    # will literally give us a list with the same thing multiple times. So
    # let's filter to unique decoy contexts, so that we don't generate the CP2
    # mutation rates a gazillion times for some weird reason...
    # (Also, let's sort this list so that there is a consistent order we can
    # rely on.)
    unique_ctxs = sorted(set(decoy_contexts))

    fancylog(
        f"Computing mutation rates for {used_decoy_contig} at these threshold "
        f"values, for each of the {len(unique_ctxs):,} decoy context(s)..."
    )
    # For each value in thresh_vals, compute the decoy genome's mutation rate.
    ctx2mr = compute_decoy_contig_mut_rates(
        contigs,
        bam_obj,
        bcf_obj,
        thresh_type,
        thresh_vals,
        used_decoy_contig,
        unique_ctxs,
        fancylog,
    )
    fancylog("Done.", prefix="")

    # At this point, we've delayed this about as long as we can -- create the
    # output directory
    misc_utils.make_output_dir(output_dir)

    fancylog(
        "Computing mutation rates and FDR estimates for the "
        f"{len(contig_name2len) - 1:,} target contig(s)..."
    )

    # Create the header for the TSV files
    tsv_header = "Contig"
    for val in thresh_vals:
        tsv_header += f"\t{thresh_type}{val}"

    # ... and write it out. The header is the same for the FDR estimate file(s)
    # and for the (# of mutations per megabase) file.
    ctx2fp = {
        ctx: os.path.join(output_dir, f"fdr-{ctx}.tsv") for ctx in unique_ctxs
    }
    num_fp = os.path.join(output_dir, "num-mutations-per-mb.tsv")
    for tsv_fp in list(ctx2fp.values()) + [num_fp]:
        with open(tsv_fp, "w") as fdr_file:
            fdr_file.write(f"{tsv_header}\n")

    # Compute FDR estimates for each target contig.
    # This is analogous to the "Full" context-dependent option for the decoy
    # genome comptuation, so we can reuse a lot of code from that.
    target_contigs = set(contig_name2len) - {used_decoy_contig}

    ctx2fdr_out = {ctx: "" for ctx in unique_ctxs}
    num_out = ""
    # Sort target contigs so that their rows in the FDR / num-per-Mb files
    # are in lexicographic order. Not really needed, but nice to have and
    # makes testing easier
    for tc_ct, target_contig in enumerate(sorted(target_contigs), 1):
        ctx2fdr_lines, num_line = compute_target_contig_fdr_curve_info(
            bcf_obj,
            thresh_type,
            thresh_vals,
            target_contig,
            contig_name2len[target_contig],
            ctx2mr,
        )

        for ctx in ctx2mr:
            ctx2fdr_out[ctx] += ctx2fdr_lines[ctx]
        num_out += num_line

        # We'll "chunk" outputs -- we'll only perform a write operation
        # every (chunk_size) lines. This way, we don't need to perform
        # write operations after processing every target contig.
        if tc_ct % chunk_size == 0:

            for ctx in ctx2mr:
                with open(ctx2fp[ctx], "a") as ff:
                    ff.write(ctx2fdr_out[ctx])
                ctx2fdr_out[ctx] = ""

            with open(num_fp, "a") as nf:
                nf.write(num_out)
            num_out = ""

    # Make one last flush if needed (yeah, i know, i know, shouldn't reuse
    # code but blhufhaosdfu)
    if num_out != "":
        for ctx in ctx2mr:
            with open(ctx2fp[ctx], "a") as ff:
                ff.write(ctx2fdr_out[ctx])

        with open(num_fp, "a") as nf:
            nf.write(num_out)

    fancylog("Done.", prefix="")

    # TODO: Verify that the decoy contig has a nonzero mutation rate?
    # If not, that's problematic, because
    # then we'd estimate the FDR as zero for every target contig. That
    # shouldn't happen most of the time, anyway. Maybe add an option to limit
    # auto-selection to just contigs with mutations? Hm, but that would be
    # annoying to implement, and users can always manually set a decoy contig.


def get_optimal_threshold_values(fi, fdr):
    """Returns the optimal values of p or r for each contig's mutation calls.

    Parameters
    ----------
    fi: pd.DataFrame
        FDR information produced by "strainFlye fdr estimate". The indices
        (rows) correspond to contigs; the columns correspond to threshold
        values of p or r (sorted in ascending order from left to right). Cells
        indicate the estimated FDR for this contig at this threshold value (and
        may be NaN if no FDR estimate was defined for a given combination of
        (contig, threshold value)).

        This should have already been sanity-checked using
        load_and_sanity_check_fdr_file(), so we don't perform any validation on
        this DataFrame's structure here.

    fdr: float
        FDR at which (non-indisputable) mutation calls for each contig will be
        fixed.

    Returns
    -------
    pd.Series
        Has the same index as fi (so, this has one entry per contig). Each
        contig's entry will be the smallest threshold value (extracted
        from the column name -- i.e. "p15" will get converted to the int 15)
        at which this contig's FDR is less than or equal to the specified fdr.
        If no such threshold value exists (i.e. all estimated FDRs for this
        contig are greater than the specified fdr and/or are NaN), the contig's
        entry in this series will be None.
    """
    # For a given contig's FDR curve, we define four cases regarding how this
    # curve intersects with a fixed FDR (represented as a vertical line).
    # The goal is to, for each contig, identify the "optimal" value of p or r
    # that maximizes the number of mutations / mb we see while keeping the
    # estimated FDR below the fixed value.
    #
    # How do we identify this "optimal" value? By looking at the FDR curve.
    # We define four "cases" of how a FDR curve can look, relative to the fixed
    # FDR (which can be thought of as an infinite vertical line).
    #     |
    # Case 1. The curve crosses the line at exactly one point.
    #     |   This is easy to handle -- select the value of p or r just below
    #     |   the intersection for this contig.
    # ^  _|_____
    # | / |
    # |/  |
    # +---|------->
    #     |
    # Case 2. The curve crosses the line more than one point.
    #     |   (This is an unfortunate possibility, because we do not have the
    #     |   guarantee that FDR estimates increase monotonically with lower
    #     |   values of p or r.) In this case, we select the value of p or r
    #     |   just below the intersection for this contig with the highest
    #     |   # mutations / mb (in the plot below, at the top intersection).
    #     |
    #     |   Notably, this will always correspond to the lowest value of p or
    #     |   r that is <= the fixed FDR: lower values of p or r also "include"
    #     |   the p- or r-mutations called from higher values of p or r, so the
    #     |   lowest "passing" threshold value will also result in the highest
    #     |   number of mutations per megabase in the target contig.
    # ^  _|_____
    # | / |
    # |/  |
    # |\__|__
    # |  _|_/
    # | / |
    # |/  |
    # +---|------->
    #     |
    # Case 3. The curve crosses the line at zero points, and all estimated FDRs
    #     |   are LOWER than the fixed FDR. In this case, just select the
    #     |   lowest value of p or r used.
    #     |
    # ^ | |
    # | | |
    # |/  |
    # +---|------->
    #     |
    # Case 4. The curve crosses the line at zero points, and all estimated FDRs
    #     |   are HIGHER than the fixed FDR. In this case, our hands are tied;
    #     |   don't select any value of p or r. We can't call any
    #     |   non-indisputable mutations for this contig, at least not at this
    #     |   fixed FDR.
    #     |
    # ^   |    /
    # |   |   /
    # |   |  |
    # +---|------->

    # Convert the FDR information to a binary matrix:
    # True  means this FDR is <= the fixed FDR value
    # False means this FDR is >  the fixed FDR value (or is a NaN)
    #
    # Note that that the ~fi.isna() check (automatically rendering all NaNs as
    # False) isn't really required, since (per IEEE-754 standards, I think? --
    # see https://stackoverflow.com/a/1573715) running "NaN <= x" should yield
    # False for all x. However, I don't really want to rely on this implicit
    # condition, and I'll take making this 0.1 seconds slower if it means I
    # don't have to think about floating-point standards any more right now.
    lte_fdr = (~fi.isna()) & (fi <= fdr)

    # While we're at it, convert the columns of this DataFrame to their integer
    # values -- so, "p15" --> 15, for example. We should have already
    # sanity-checked this DataFrame's structure before calling this.
    lte_fdr.columns = fi.columns.str.slice(1).astype(int)

    def get_optimal_threshold(fdr_df_row):
        # We implicitly go through from lower to higher threshold values -- so
        # we'll select the lowest threshold value that is <= the fixed FDR.
        # This automatically accounts for Cases 1, 2, and 3 above.
        for ci, val in enumerate(fdr_df_row):
            if val:
                return fdr_df_row.index[ci]
        # If we make it here, then none of the threshold values were <= the
        # fixed FDR. So this is a "Case 4" situation -- return NaN.
        return np.nan

    # We can select each optimal threshold using DataFrame.apply(). This is
    # faster than naive looping through the DataFrame, but it could still
    # probably be sped up (although I'm not sure how exactly we could use
    # vectorization here). See
    # https://web.archive.org/web/20181106230656/https://engineering.upside.com/a-beginners-guide-to-optimizing-pandas-code-for-speed-c09ef2c6a4d6
    # for details.
    return lte_fdr.apply(get_optimal_threshold, axis=1)


def write_filtered_bcf(
    in_bcf_obj,
    out_bcf_fp,
    decoy_contig,
    optimal_thresh_vals,
    thresh_type,
    thresh_high,
    fancylog,
    verbose,
):
    """Writes out a filtered BCF based on optimal threshold values.

    The filtered BCF will include:

        1. "Indisputable" mutations (that pass thresh_high). We will include
           indisputable mutations from all contigs -- the target(s), and the
           decoy.

        2. Non-indisputable mutations (that do not pass thresh_high but pass
           the corresponding optimal_thresh_vals value for a target contig).
           We will only include these mutations from the target contigs, since
           we don't have an optimal threshold value generated for the decoy
           contig.

           If a target contig did not have an optimal threshold value given
           (i.e. its OTV is np.nan), then we will not include any
           non-indisputable mutations for this contig.

    Parameters
    ----------
    in_bcf_obj: pysam.VariantFile
        Object describing a BCF file produced by strainFlye's naive calling.

    out_bcf_fp: str
        Filepath to which a filtered version of in_bcf_obj will be written.

    decoy_contig: str
        Name of the decoy contig.

    optimal_thresh_vals: pd.Series
        Output of get_optimal_threshold_values(). The index corresponds to
        contig names; the values correspond to either the optimal value of p
        or r for a contig (expressed as a number), or np.nan (if no optimal
        value exists for this contig -- i.e. all of this contig's estimated
        FDRs were greater than the threshold or were undefined).

        Note that the dtype of this Series will probably be a float, because
        the presence of NaNs forces the dtype to be float. (This problem is
        documented, for example, at
        https://pandas.pydata.org/docs/user_guide/integer_na.html.) This isn't
        a problem, since I know that all of the numbers are integers (e.g.
        15.0) -- so we can call int() on them without worrying.

    thresh_type: str
        Either "p" or "r". We assume at this point that in_bcf_obj,
        optimal_thresh_vals, etc. all also use this thresh_type -- so please
        don't break this, ok?

    thresh_high: int
        The "high" (indisputable) value of p or r passed to
        "strainFlye fdr estimate".

    fancylog: function
        Logging function.

    verbose: bool
        If True, logs information about the number of mutations preserved for
        each contig.

    Returns
    -------
    None
    """
    # We will write out certain mutation "records" to this BCF.
    out_bcf_obj = pysam.VariantFile(out_bcf_fp, "wb", header=in_bcf_obj.header)

    # For each contig (considering both the decoy and target(s))...
    for contig in in_bcf_obj.header.contigs:

        num_written_muts = 0
        num_original_muts = 0

        thresh_used = thresh_high
        # We only call "indisputable" mutations for the decoy contig.
        if contig != decoy_contig:
            # Figure out the minimum threshold to use for this contig. If we
            # can only output indisputable mutations for this contig, then this
            # threshold will remain thresh_high; if we have a lower threshold
            # value defined, we can use that. (We can do this because mutations
            # that pass thresh_high will automatically also pass lower
            # thresholds.)
            otv = optimal_thresh_vals[contig]
            if not np.isnan(otv):
                thresh_used = otv

        for mut in in_bcf_obj.fetch(contig):

            alt_pos = mut.info.get("AAD")[0]
            cov_pos = mut.info.get("MDP")

            # We can figure out whether or not this mutation passes thresh_used
            # by using the calling functions from call_utils. Notably, we pass
            # 1 as min_alt_pos for call_p_mutation() because we don't care
            # about min_alt_pos at this point -- like, it applied to the
            # mutations the user screened for using "strainFlye call"
            is_mut = False
            if thresh_type == "p":
                is_mut = call_utils.call_p_mutation(
                    alt_pos, cov_pos, thresh_used, 1
                )
            else:
                is_mut = call_utils.call_r_mutation(alt_pos, thresh_used)

            if is_mut:
                out_bcf_obj.write(mut)
                num_written_muts += 1

            num_original_muts += 1

        if verbose:
            fancylog(
                (
                    f"Wrote out {num_written_muts:,} / {num_original_muts:,} "
                    f"mutations for {contig}."
                ),
                prefix="",
            )


def load_and_sanity_check_fdr_file(fdr_info, thresh_type):
    """Loads and sanity-checks a FDR TSV file.

    This sanity-checks the structure of the file -- it doesn't say anything
    about the contigs all matching up with a BCF file, for example. The main
    goal is just checking that this looks like the sort of TSV file that the
    estimate command would have generated.

    Parameters
    ----------
    fdr_info: str
        Filepath to a FDR TSV file.

    thresh_type: str
        Either "p" or "r", depending on which type of mutations were called.

    Returns
    -------
    fi: pd.DataFrame
        DataFrame containing the information stored in the TSV file. Indices
        (rows) correspond to contigs; columns correspond to threshold values
        of p or r (sorted in ascending order from left to right). Cells
        indicate the estimated FDR for this contig at this threshold value.

    Raises
    ------
    ParameterError
        If the TSV file seems malformed.

    Other errors
        Can be raised by pd.read_csv() if the file seems malformed.
        We don't attempt to catch these errors.
    """
    fi = pd.read_csv(fdr_info, sep="\t", index_col=0)
    # error prefix -- saves us some typing...
    ep = "Input FDR TSV file seems malformed"
    if fi.index.name != "Contig":
        raise ParameterError(f'{ep}: no "Contig" header?')
    if len(fi.index) < 1:
        raise ParameterError(f"{ep}: no contigs described?")
    if len(fi.columns) < 1:
        raise ParameterError(f"{ep}: no threshold values described?")

    prev_col_val = None
    for col in fi.columns:
        if col[0] != thresh_type:
            raise ParameterError(
                f"{ep}: columns should start with {thresh_type}."
            )
        col_val = int(col[1:])
        if prev_col_val is not None:
            if col_val <= prev_col_val:
                raise ParameterError(
                    f"{ep}: values of {thresh_type} should increase from left "
                    "to right."
                )
            # We could eventually support different step sizes, but not yet.
            # Actually I think this might be overzealous -- I don't think we do
            # anything in "fix" that requires known step sizes -- but whatever,
            # I wanna be careful.
            if col_val != prev_col_val + 1:
                raise ParameterError(
                    f"{ep}: values of {thresh_type} should only increase in "
                    "steps of 1."
                )
        prev_col_val = col_val

        col_fdrs = fi[col]
        # Check that this column is numeric (NaNs are ok).
        # See https://stackoverflow.com/a/45568283.
        if not pd.api.types.is_numeric_dtype(col_fdrs):
            raise ParameterError(
                f"{ep}: Column {col} doesn't seem to be numeric?"
            )

        # At this point, we've already screened for non-numeric dtypes, so we
        # should be able to use < 0 without problems
        if (col_fdrs < 0).any():
            raise ParameterError(
                f"{ep}: Column {col} contains negative FDR estimates?"
            )

    return fi


def log_optimal_threshold_value_stats(
    optimal_thresh_vals, thresh_type, thresh_min, thresh_max, fdr, fancylog
):
    """Logs information about the optimal threshold values.

    Selecting these values is arguably the main part of how "strainFlye fdr
    fix" works, so I wanted to be as clear as possible -- hence this function
    existing.

    Parameters
    ----------
    optimal_thresh_vals: pd.Series
        Output of get_optimal_threshold_values(). The index corresponds to
        contig names; the values correspond to either the optimal value of p
        or r for a contig (expressed as a number), or np.nan (if no optimal
        value exists for this contig -- i.e. all of this contig's estimated
        FDRs were greater than the threshold or were undefined).

        Note that the dtype of this Series will probably be a float, because
        the presence of NaNs forces the dtype to be float. (This problem is
        documented, for example, at
        https://pandas.pydata.org/docs/user_guide/integer_na.html.) This isn't
        a problem, since I know that all of the numbers are integers (e.g.
        15.0) -- so we can call int() on them without worrying.

    thresh_type: str
        Either "p" or "r".

    thresh_min: int
        Minimum value of p or r used in "strainFlye call".

    thresh_max: int
        The maximum value of p or r: this is equal to the "high" (indisputable)
        value of p or r passed to "strainFlye fdr estimate" minus 1. Note that
        this can technically be equal to thresh_min in the silly case where
        --high-p = --min-p + 1 (same goes for r), although that really
        shouldn't happen in practice.

    fdr: float
        FDR at which (non-indisputable) mutation calls for each contig will be
        fixed.

    fancylog: function
        Logging function.

    Returns
    -------
    None
    """
    # A contig can have a N/A optimal threshold value if its entire FDR curve
    # was > the fixed FDR (see Case 4 in get_optimal_threshold_values()).
    nonna = optimal_thresh_vals[~optimal_thresh_vals.isna()]
    num_nonna = len(nonna)
    num_targets = len(optimal_thresh_vals)

    if num_nonna == 0:
        # This should not happen except for really weird cases
        fancylog(
            (
                f"Warning: No values of {thresh_type} resulted in estimated "
                f"FDRs \u2264 the fixed FDR, for all {num_targets:,} contigs."
            ),
            prefix="",
        )
    else:
        fancylog(
            (
                f"For {num_nonna:,} / {num_targets:,} contigs, there exist "
                f"values of {thresh_type} (at least, considering the range "
                f"from {thresh_type} = {thresh_min} to {thresh_type} = "
                f"{thresh_max}) that yield estimated FDRs \u2264 {fdr}%."
            ),
            prefix="",
        )
        # NOTE: Could roll these into a single loop to speed this up, but this
        # almost certainly won't be a bottleneck
        # Also see above re: int() calls -- all of the numbers here should ints
        # that just end with ".0", so we're not actually losing any information
        min_contig = nonna.idxmin()
        min_tv = int(nonna[min_contig])
        max_contig = nonna.idxmax()
        max_tv = int(nonna[max_contig])
        mean_tv = mean(nonna)
        fancylog(
            (
                f"These values range from {thresh_type} = {min_tv:,} "
                f"({min_contig}) to {thresh_type} = {max_tv:,} ({max_contig})."
            ),
            prefix="",
        )
        fancylog(
            f"The mean of these values is {thresh_type} = {mean_tv:,.2f}.",
            prefix="",
        )


def run_fix(bcf, fdr_info, fdr, output_bcf, fancylog, verbose):
    """Performs FDR fixing.

    Parameters
    ----------
    bcf: str
        Filepath to a BCF file generated by one of strainFlye call's
        subcommands.

    fdr_info: str
        Filepath to a TSV file describing estimated FDRs for the target
        contigs.

    fdr: int
        False Discovery Rate (FDR) to fix mutation calls at. Scaled up by 100,
        like values of p are elsewhere -- so fdr = 100 indicates an FDR of 1%.

    output_bcf: str
        Filepath to which we will write an (indexed) BCF file. This will
        contain a subset of the mutations described in the input BCF file.

    fancylog: function
        Logging function.

    verbose: bool
        If True, display information about each contig while writing the
        filtered BCF. At least on the SheepGut dataset, this step takes the
        longest, and it can be useful to allow verbosity there.

    Returns
    -------
    None

    Raises
    ------
    ParameterError
        - If the set of contigs in the FDR TSV file is not a subset of those
          in the BCF file.
        - If there is not exactly one contig that is present in the BCF file
          but not in the FDR TSV file.

    - bcf_utils.parse_sf_bcf() can also raise various errors if the input BCF
      is malformed.
    """
    fancylog("Loading and checking BCF and TSV files...")
    # Like in run_estimate(): Load the BCF file and figure out what contigs it
    # describes
    bcf_obj, thresh_type, thresh_min = bcf_utils.parse_sf_bcf(bcf)
    bcf_contigs = set(bcf_obj.header.contigs)

    # Load the estimated FDR file.
    fi = load_and_sanity_check_fdr_file(fdr_info, thresh_type)
    tsv_desc = "the FDR TSV file"
    # Ensure that the contigs described in these TSV files (tsv_contigs)
    # are all described in the BCF file. We don't check for an exact match,
    # because the decoy contig will be missing.
    tsv_contigs = set(fi.index)
    misc_utils.verify_contig_subset(
        tsv_contigs, bcf_contigs, tsv_desc, "the BCF file"
    )
    absent_contigs = bcf_contigs - tsv_contigs
    if len(absent_contigs) != 1:
        # Fun thing about this error message: we can say "contigs" because this
        # error fires if and only if len(absent_contigs) is not 1 :D
        raise ParameterError(
            "Exactly one contig in the BCF file (the decoy contig) should be "
            f"missing from {tsv_desc}. However, {len(absent_contigs):,} "
            "contigs are missing!"
        )

    # If we've made it here, we know there is exactly one contig in the BCF but
    # not in the FDR TSV file. This must be the decoy contig.
    decoy_contig = absent_contigs.pop()
    fancylog(
        f"Looks good so far; decoy contig seems to be {decoy_contig}.",
        prefix="",
    )

    # Figure out the "indisputable" mutation cutoff.
    # At this point, we already know that the FDR info file is structured as
    # expected (e.g. columns are all formatted like "p15", "p16", etc.), so we
    # can slice off the first character from the last column without worrying
    # about the file being structured incorrectly.
    thresh_max = int(fi.columns[-1][1:])
    thresh_high = thresh_max + 1
    fancylog(
        (
            'Looks like the cutoff for "indisputable" mutations was '
            f"{thresh_type} = {thresh_high:,}."
        ),
        prefix="",
    )
    fancylog(
        (
            "All mutations passing this cutoff will be included in the "
            "output BCF file."
        ),
        prefix="",
    )

    fancylog(
        "Based on the FDR information, finding optimal values of "
        f"{thresh_type} for each contig..."
    )
    # We can now begin in earnest. Figure out the "optimal" value of p or r for
    # for each contig, based on the estimated FDR information.
    optimal_thresh_vals = get_optimal_threshold_values(fi, fdr)
    fancylog("Done.", prefix="")
    log_optimal_threshold_value_stats(
        optimal_thresh_vals, thresh_type, thresh_min, thresh_max, fdr, fancylog
    )

    # VCF / BCF headers aren't (currently) editable using pysam
    # (https://github.com/pysam-developers/pysam/issues/668), but we can
    # fortunately use "bcftools annotate" to do this the long(ish) way -- write
    # the "first" BCF file to a temporary file, then annotate it, and output
    # the annotated BCF to the final "output BCF" location. (Annotation doesn't
    # seem to be doable "in place" using bcftools.)
    with tempfile.NamedTemporaryFile(
        mode="w+b"
    ) as temp_bcf_file, tempfile.NamedTemporaryFile(mode="w+") as header_file:
        fancylog(
            "Writing a filtered BCF file (to a temporary location, for now) "
            "including both "
            "(1) indisputable mutations from all contigs and "
            "(2) non-indisputable mutations from the target contigs that "
            f"result in a FDR \u2264 {fdr}%..."
        )
        # Filter mutations for each contig to those that pass these thresholds
        # (in addition to indisputable mutations that pass thresh_high).
        write_filtered_bcf(
            bcf_obj,
            temp_bcf_file.name,
            decoy_contig,
            optimal_thresh_vals,
            thresh_type,
            thresh_high,
            fancylog,
            verbose,
        )
        fancylog("Done.", prefix="")

        fancylog(
            "Updating the filtered BCF file's header to indicate that FDR "
            "fixing was done..."
        )
        # We don't have information about the decoy context (Full, CP2, ...)
        # but that's not a big deal. the main thing is just making it clear
        # that this file has been updated from the naive calls
        new_header_line = (
            f"##FILTER=<ID=strainFlye_fdr_fix_fdr_{fdr}_decoy_{decoy_contig}_"
            f'high_{thresh_type}_{thresh_high}, description="Mutations '
            'filtered to a fixed FDR">'
        )
        with open(header_file.name, "w") as hf:
            hf.write(new_header_line)
        # Update the header in the temp BCF file, and -- while this is
        # happening -- write out the updated BCF file to the output location
        subprocess.run(
            [
                "bcftools",
                "annotate",
                "-h",
                header_file.name,
                "-O",
                "b",
                "-o",
                output_bcf,
                temp_bcf_file.name,
            ],
            check=True,
        )
        fancylog("Done.", prefix="")

        # just gotta index the BCF, then we're done!
        bcf_utils.index_bcf(output_bcf, fancylog)
