# Utilities for strainFlye fdr.


import tempfile
import subprocess
import pysam
import numpy as np
import pandas as pd
from math import floor
from statistics import mean
from collections import defaultdict
from strainflye import fasta_utils, call_utils, misc_utils, bcf_utils
from strainflye.bcf_utils import parse_sf_bcf
from .errors import ParameterError, SequencingDataError
from .config import DI_PREF


def check_decoy_selection(diversity_indices, decoy_contig):
    """Checks that only one of (diversity index file, decoy contig) is given.

    Parameters
    ----------
    diversity_indices: str or None
        If a str, this should be a filepath to a TSV file representing
        diversity index info.

    decoy_contig: str or None
        If a str, this should be the name of a contig described in the
        BCF file.

    Returns
    -------
    selection_type: str
        Will be "DI" if only diversity_indices is not None, and will be "DC" if
        only decoy_contig is not None.

    Raises
    ------
    ParameterError
        - If diversity_indices and decoy_contig are both not None
        - If diversity_indices and decoy_contig are both None
    """
    di = diversity_indices is not None
    dc = decoy_contig is not None

    if di:
        if dc:
            raise ParameterError(
                "Both the diversity indices file and a decoy contig are "
                "specified. These options are mutually exclusive."
            )
        else:
            return "DI"
    else:
        if dc:
            return "DC"
        else:
            raise ParameterError(
                "Either the diversity indices file or a decoy contig must be "
                "specified."
            )


def normalize_series(in_series):
    """Converts a series to values in the range [0, 1].

    Parameters
    ----------
    in_series: pd.Series
        We assume that this does not contain any nan values.

    Returns
    -------
    None or pd.Series
        If the minimum and maximum of in_series are identical, this will just
        return None. (In this case, we can't scale values, because the
        denominator we use when converting a value (max - min) is zero.)

        If the minimum and maximum are not identical (which should usually
        be the case with diversity indices, hopefully...) then this will return
        a pd.Series with the same index as in_series, but with each entry
        scaled to within the range [0, 1] (such that the min value in in_series
        is set to 0, the max is set to 1, and everything else is in between).
    """
    # Small TODO: in theory, it'd be faster to combine the
    # computation of min and max into a single pass over the values
    # (see e.g. https://stackoverflow.com/q/12200580) but this
    # probably won't be a performance bottleneck so I'm not gonna
    # bother for now
    min_val = min(in_series)
    max_val = max(in_series)
    if min_val == max_val:
        return None
    else:
        # Use pandas' vectorization to apply linear interpolation
        # across all diversity indices in this Series
        return (in_series - min_val) / (max_val - min_val)


def autoselect_decoy(diversity_indices, min_len, min_avg_cov, fancylog):
    """Attempts to select a good decoy contig based on diversity index data.

    There are lots of ways to implement this, so here we just stick with
    something simple that combines the diversity index information from
    multiple thresholds:

    1. Filter to all contigs whose lengths and average coverages meet the
       specified thresholds. The number of "passing" contigs is C. If C = 1,
       select this contig; if C = 0, raise an error.

    2. For each of the D diversity index columns provided in the file (where at
       least two contigs have defined diversity indices), compute the minimum
       and maximum diversity index in this column. Assign each contig a score
       for this column in [0, 1] using linear interpolation: the contig with
       the lowest diversity index gets a score of 0, the contig with the
       highest gets a score of 1, and everything else is scaled in between.
       If a contig has an undefined diversity index in such a column, set its
       score for this column to 1.

    3. For each of the C contigs, sum scores across all of the D diversity
       index columns. Select the contig with the lowest score sum. Break ties
       arbitrarily.

    Parameters
    ----------
    diversity_indices: str
        Filepath to a TSV file containing diversity index info, generated by
        one of strainFlye call's subcommands. In addition to diversity index
        values, this also includes length and average coverage information
        for each contig in the file (this will help a lot, since computing
        these values if we don't already have them is time-consuming or at
        the very least annoying).

    min_len: int
        In order for a contig to be selected as the decoy, its length must be
        at least this.

    min_avg_cov: float
        In order for a contig to be selected as the decoy, its average coverage
        must be at least this.

    fancylog: function
        Logging function.

    Returns
    -------
    decoy_contig: str
        The name of the decoy contig we find.

    Raises
    ------
    ParameterError
        If the diversity index file:
        - Describes < 2 contigs (should have already been caught during align,
          but you never know)
        - Doesn't have Length or AverageCoverage columns

    SequencingDataError
        - If none of the contigs in the diversity index file pass the length
          and average coverage thresholds.
        - If none of the diversity index columns has at least two "passing"
          contigs with defined and distinct diversity indices in this column.
    """
    # We require that the diversity indices file describes at least two
    # contigs, so that -- once we select one contig as a decoy -- we still have
    # at least one target contig left!
    di = misc_utils.load_and_sanity_check_diversity_indices(
        diversity_indices, min_num_contigs=2
    )

    # Filter to contigs that pass both the length and coverage thresholds.
    # https://stackoverflow.com/a/13616382
    passing_di = di[
        (di["Length"] >= min_len) & (di["AverageCoverage"] >= min_avg_cov)
    ]
    passing_contigs = passing_di.index
    num_passing_contigs = len(passing_contigs)
    # it isn't clear how much precision min_avg_cov has, so we don't impose any
    # limit on how many digits it goes out to when printing it out. let's let
    # python handle this one
    check_str = (
        f"the min length \u2265 {min_len:,} and min average cov \u2265 "
        f"{min_avg_cov:,}x checks"
    )
    if num_passing_contigs == 0:
        raise SequencingDataError(f"No contigs pass {check_str}.")
    elif num_passing_contigs == 1:
        # Arguably, we could raise an error here -- but we know at this point
        # that there are >= 2 contigs in the file (and this is just the only
        # one that passes the length and coverage thresholds). So we may as
        # well select this contig, albeit after giving a warning.
        fancylog(
            f"Warning: Only one contig passes {check_str}. Selecting it.",
            prefix="",
        )
        return passing_contigs[0]

    # Actually start scoring contigs based on their diversity indices.
    contig2score = defaultdict(int)
    # Diversity index columns where there are at least two "passing" contigs
    # that have defined (non-NA) diversity indices
    good_di_cols = []
    for di_col in passing_di.columns:

        # ignore non-diversity-index columns
        if di_col.startswith(DI_PREF):
            di_vals = passing_di[di_col]

            # Ignore diversity index columns where less than two contigs have
            # defined diversity indices, since these don't mean much for our
            # score computation (at least as currently defined)
            finite_di_vals = di_vals[~di_vals.isna()]
            if len(finite_di_vals.index) >= 2:

                scores = normalize_series(finite_di_vals)
                # normalize_series() will return None if the min and max value
                # in finite_di_vals are identical. In this case, we can't
                # generate meaningful scores, so we just move on.
                if scores is not None:
                    good_di_cols.append(di_col)

                    # Update scores.
                    for passing_contig in passing_di.index:
                        if passing_contig in scores:
                            contig2score[passing_contig] += scores[
                                passing_contig
                            ]
                        else:
                            # Penalize this contig for not having a defined
                            # diversity index in this column: give it the max
                            # possible score
                            contig2score[passing_contig] += 1

    if len(good_di_cols) == 0:
        raise SequencingDataError(
            "No diversity index column has at least two contigs that (1) pass "
            f"{check_str} and (2) have defined and distinct diversity indices "
            "in this column."
        )
    # Find the passing contig with the lowest total score:
    # https://stackoverflow.com/a/3282904
    lowest_score_contig = min(passing_contigs, key=contig2score.get)
    return lowest_score_contig


def compute_number_of_mutations_in_full_contig(
    bcf_obj, thresh_type, thresh_vals, contig
):
    """Counts mutations at certain p or r thresholds in a contig.

    This function is designed to be useful for either decoy or target contigs.

    We perform some sanity checking on thresh_vals, but we'll assume that
    the other parameters are well-formed (e.g. thresh_type is either "p" or
    "r", contig is in bcf_obj, etc.)

    Parameters
    ----------
    bcf_obj: pysam.VariantFile
        Object describing a BCF file produced by strainFlye's naive calling.

    thresh_type: str
        Either "p" or "r", depending on which type of mutations were called in
        bcf_obj.

    thresh_vals: range
        Range of values of p or r (depending on thresh_type) at which to
        count mutations in this contig. Must use a step size of 1.
        If a mutation is a mutation for a value of p or r larger than the
        maximum p or r value here (i.e. it's "indisputable"), it will not be
        counted.

    contig: str
        Name of a contig for which mutation rates will be computed.

    Returns
    -------
    num_muts: list
        List with the same length as thresh_vals. The i-th value in this list
        describes the number of called mutations for the i-th threshold in
        thresh_vals.

    Raises
    ------
    ParameterError
        If thresh_vals doesn't use a step size of 1.
        If len(thresh_vals) is zero.
        If either the stop or start of thresh_vals are zero or below.

        (None of these should happen in practice, but you never know...)
    """
    if thresh_vals.step != 1:
        raise ParameterError("thresh_vals must use a step size of 1.")
    if len(thresh_vals) <= 0:
        raise ParameterError("thresh_vals must have a positive length.")
    if thresh_vals.start <= 0 or thresh_vals.stop <= 0:
        raise ParameterError("thresh_vals' start and stop must be positive.")

    # For each threshold value, keep track of how many mutations we've seen at
    # this threshold.
    num_muts = [0] * len(thresh_vals)

    # We can just infer the "indisputable" mutation value from thresh_vals as
    # the value just above the max thresh_vals entry
    high_val = thresh_vals[-1] + 1
    min_val = thresh_vals[0]

    for mut in bcf_obj.fetch(contig):

        # AAD is technically a tuple since it's defined once for every alt
        # allele, but r/n strainflye call only produces max one alt allele per
        # mutation. So it's a tuple with 1 element (at least for now).
        alt_pos = mut.info.get("AAD")[0]
        cov_pos = mut.info.get("MDP")

        if thresh_type == "p":
            max_passing_val = floor((10000 * alt_pos) / cov_pos)
        else:
            max_passing_val = alt_pos

        # Don't count "indisputable" mutations towards mutation rates
        if max_passing_val >= high_val:
            continue

        # NOTE: This is already more optimized than the analysis
        # notebooks, but I think it could still be made faster. Maybe
        # just increment a single value (corresponding to the max
        # passing p/r), and then do everything at the end after seeing
        # all mutations in one pass? Doesn't seem like a huge bottleneck tho.
        num_vals_to_update = max_passing_val - min_val + 1
        for i in range(num_vals_to_update):
            num_muts[i] += 1
    return num_muts


def compute_full_decoy_contig_mut_rates(
    bcf_obj, thresh_type, thresh_vals, decoy_contig, decoy_contig_len
):
    """Computes mutation rates for the entirety of a decoy contig.

    This is designed for the "Full" option, in which we consider every position
    in the contig as a part of the decoy.

    Parameters
    ----------
    bcf_obj: pysam.VariantFile
        Object describing a BCF file produced by strainFlye's naive calling.

    thresh_type: str
        Either "p" or "r", depending on which type of mutations were called in
        bcf_obj.

    thresh_vals: range
        Range of values of p or r (depending on thresh_type) at which to
        compute mutation rates for this contig. Must use a step size of 1.
        If a mutation is a mutation for a value of p or r larger than the
        maximum p or r value here (i.e. it's "indisputable"), it will not be
        included in the mutation rate computation.

    decoy_contig: str
        Decoy contig name.

    decoy_contig_len: int
        Decoy contig sequence length.

    Returns
    -------
    mut_rates: list
        Mutation rates for each threshold value in thresh_vals.
    """
    num_muts = compute_number_of_mutations_in_full_contig(
        bcf_obj, thresh_type, thresh_vals, decoy_contig
    )
    denominator = 3 * decoy_contig_len
    return [n / denominator for n in num_muts]


def compute_target_contig_fdr_curve_info(
    bcf_obj,
    thresh_type,
    thresh_vals,
    target_contig,
    target_contig_len,
    decoy_mut_rates,
):
    """Computes FDR curve information for a given target contig.

    The intent is to create output that can be dropped straight into a TSV
    file, without too much work on the part of the caller.

    Parameters
    ----------
    bcf_obj: pysam.VariantFile
        Object describing a BCF file produced by strainFlye's naive calling.

    thresh_type: str
        Either "p" or "r", depending on which type of mutations were called in
        bcf_obj.

    thresh_vals: range
        Range of values of p or r (depending on thresh_type) at which to
        count mutations in this contig. Must use a step size of 1.
        If a mutation is a mutation for a value of p or r larger than the
        maximum p or r value here (i.e. it's "indisputable"), it will not be
        included in the FDR curve information computation.

    target_contig: str
        Name of the target contig.

    target_contig_len: int
        Target contig sequence length.

    decoy_mut_rates: list
        List of mutation rates (with the same length as thresh_vals) for the
        decoy contig. The context used to compute these mutation rates (Full,
        CP2, Nonsyn, ...) doesn't matter. All we care about is these rates.

    Returns
    -------
    (fdr_line, num_line): (str, str)
        These are both tab-separated lines (suitable for adding to a TSV file
        where the first column is the target contig name and there are
        len(thresh_vals) additional columns).

        The first entry in fdr_line and num_line is the target contig name.
        The remaining entries in fdr_line describe the estimated FDR for this
        target contig at each threshold value (the x-axis on the FDR curves, as
        we currently draw them). The remaining entries in num_line describe the
        number of mutations per megabase for this target contig at each
        threshold value (the y-axis on the FDR curves, as we currently draw
        them).
    """
    num_muts = compute_number_of_mutations_in_full_contig(
        bcf_obj, thresh_type, thresh_vals, target_contig
    )

    # it's a long story. see docs for compute_num_mutations_per_mb() in
    # https://github.com/fedarko/sheepgut/blob/main/notebooks/DemonstratingTargetDecoyApproach.ipynb
    numpermb_coeff = 1000000 / target_contig_len
    denominator = 3 * target_contig_len
    # The 100 is because we convert FDRs from [0, 1] to percentages
    # (technically, FDRs can exceed 100% if the decoy mutation rate > the
    # target mutation rate, but that shouldn't happen too often)
    fdr_coeff = 100 * denominator

    fdr_line = target_contig
    num_line = target_contig
    # TODO: Maybe speed this up by first creating a pandas DataFrame of all
    # contigs and their num_muts, then using vectorization or apply() to do
    # this stuff on all contigs at once? But this runs in ~2 minutes on the
    # entire SheepGut dataset, so it's not a substantial bottleneck.
    for i, n in enumerate(num_muts):
        if n == 0:
            # The FDR is undefined if the target's mutation rate is zero
            fdr_val = "NA"
            num_val = "0"
        else:
            fdr_val = str((fdr_coeff * decoy_mut_rates[i]) / n)
            num_val = str(numpermb_coeff * n)
        fdr_line += f"\t{fdr_val}"
        num_line += f"\t{num_val}"
    return fdr_line + "\n", num_line + "\n"


def compute_decoy_contig_mut_rates(
    contigs,
    bcf_obj,
    thresh_type,
    thresh_vals,
    decoy_contig,
    decoy_context,
):
    """Computes mutation rates for a decoy contig at some threshold values.

    Parameters
    ----------
    contigs: str
        Filepath to a FASTA file containing contigs in which mutations were
        naively called. We'll only really use this to extract the decoy
        contig's sequence (it's probably easier to load it here then to rely on
        the caller to load it).

    bcf_obj: pysam.VariantFile
        Object describing a BCF file produced by strainFlye's naive calling.

    thresh_type: str
        Either "p" or "r", depending on which type of mutations were called in
        bcf_obj.

    thresh_vals: range
        Range of values of p or r (depending on thresh_type) at which to
        compute mutation rates. Must use a step size of 1.

    decoy_contig: str
        Name of a contig to compute mutation rates for.

    decoy_context: str
        Context-dependent mutation settings to apply in computing the mutation
        rates. One of the following:
         - "Full": Compute mutation rates across the entire contig.
         - "CP2": Only consider positions that are 1) located in a single
                  predicted protein-coding gene and 2) are located in the
                  second codon position of this gene.
         - "Nonsyn": Compute mutation rates based on treating potential
                     nonsynonymous mutations (for positions located in a single
                     predicted protein-coding gene) as a decoy.
         - "Nonsense": Like Nonsyn, but for nonsense mutations.
         - "CP2Nonsyn": Nonsyn, but only for positions in CP2.
         - "CP2Nonsense": Nonsense, but only for positions in CP2.

    Returns
    -------
    decoy_mutation_rates: list
        Has the same dimensions as thresh_vals. The i-th value of
        decoy_mutation_rates corresponds to the mutation rate computed for this
        decoy contig based on naive calling using the i-th threshold value.

    Raises
    ------
    SequencingDataError
        If decoy_contig isn't in the contigs. (This should never happen if
        this function is called from run_estimate(), which should already have
        ensured that this is the case.)
    """
    decoy_seq = fasta_utils.get_single_seq(contigs, decoy_contig)

    if decoy_context == "Full":
        return compute_full_decoy_contig_mut_rates(
            bcf_obj,
            thresh_type,
            thresh_vals,
            decoy_contig,
            len(decoy_seq),
        )
    else:
        raise NotImplementedError(
            'Only the "Full" context is implemented now.'
        )

    # TODO:
    # - If decoy_context != "Full",
    #   - Predict genes in this sequence using prodigal. Save .sco to tempfile.
    # - Fetch mutations aligned to this contig in the BCF file.
    # -


def run_estimate(
    contigs,
    bcf,
    diversity_indices,
    decoy_contig,
    decoy_context,
    high_p,
    high_r,
    decoy_min_length,
    decoy_min_average_coverage,
    output_fdr_info,
    output_num_info,
    fancylog,
    chunk_size=500,
):
    """Performs decoy selection and FDR estimation.

    Notably, both high_p and high_r will be defined regardless of if the BCF
    was generated using p- or r-mutations, because (unlike strainFlye call)
    I don't think it's worth splitting this step up into two sub-commands
    by p- or r-mutations. So we'll just ignore one of these values.

    Parameters
    ----------
    contigs: str
        Filepath to a FASTA file containing contigs in which mutations were
        naively called.

    bcf: str
        Filepath to a BCF file generated by one of strainFlye call's
        subcommands.

    diversity_indices: str or None
        If a str, this should be a filepath to a TSV file containing diversity
        index info, also generated by one of strainFlye call's subcommands.
        We'll use this information to automatically select a decoy contig.

    decoy_contig: str or None
        If a str, this should be the name of a contig described in the
        BCF file. We'll use this as a decoy contig.

    decoy_context: str
        Context-dependent mutation settings (e.g. Full, CP2, Nonsyn, ...)
        Thankfully, we know this will be one of a set of allowed choices,
        since we use click.Choice() to screen it at the CLI.

    high_p: int
        "Indisputable" threshold for p-mutations (scaled up by 100).

    high_r: int
        "Indisputable" threshold for r-mutations.

    decoy_min_length: int
        If automatically selecting decoy contigs, we'll only consider contigs
        that are at least this long.

    decoy_min_average_coverage: float
        If automatically selecting decoy contigs, we'll only consider contigs
        with average coverages of at least this.

    output_fdr_info: str
        Filepath to which we'll write a TSV file describing estimated FDRs
        for the target contigs. (x-axis values for the FDR curves, at least as
        plotted in the paper.)

    output_num_info: str
        Filepath to which we'll write a TSV file describing numbers of
        mutations per megabase. (y-axis values for the FDR curves.)

    fancylog: function
        Logging function.

    chunk_size: int
        After seeing this many target contigs, we'll write out their FDR and (#
        mutations per Mb) information to the corresponding files.

    Returns
    -------
    None

    Raises
    ------
    ParameterError
        - If the selected decoy contig isn't present in the contigs file.
        - If the high p (or high r) threshold is <= the minimum threshold value
          used in the BCF file.
        - If the BCF file's contigs do not exactly match the FASTA file's
          contigs.

    - parse_sf_bcf() can also raise various errors if the input BCF is
      malformed.
    - fasta_utils.get_name2len() can also raise errors if the input FASTA file
      of contigs is malformed.
    """
    fancylog("Loading and checking FASTA and BCF files...")

    # get name -> length mapping for the FASTA file; also sanity check it a bit
    contig_name2len = fasta_utils.get_name2len(contigs, min_num_contigs=2)

    # Load the BCF file, also
    bcf_obj, thresh_type, thresh_min = parse_sf_bcf(bcf)

    # Figure out which contigs are considered in the BCF file
    # (thankfully, this header can include contigs with no called mutations,
    # which makes my life easier here)
    bcf_contigs = set(bcf_obj.header.contigs)

    # Ensure that the sets of contigs in the BCF file and FASTA match exactly
    # (In theory, we could allow the BCF to be a subset of the FASTA, but...
    # nah, that's too much work and the user should already have an exactly-
    # matching FASTA file around from when they ran "call".)
    misc_utils.verify_contig_subset(
        bcf_contigs,
        set(contig_name2len),
        "the BCF file",
        "the FASTA file",
        exact=True,
    )
    misc_utils.verify_contig_lengths(contig_name2len, bcf_obj=bcf_obj)
    # We *could* try to ensure that the diversity index file's contigs, if
    # a diversity index file is specified, are a subset of the BCF -- but
    # no need to do this extra work right now. the main thing that matters IMO
    # is just checking that the selected decoy contig is in the BCF, which is
    # much easier to do later.
    fancylog(
        (
            "The FASTA and BCF files describe the same "
            f"{len(contig_name2len):,} contigs."
        ),
        prefix="",
    )
    fancylog(
        f"Also, the input BCF file contains {thresh_type}-mutations "
        f"(minimum {thresh_type} = {thresh_min:,}).",
        prefix="",
    )

    # Identify decoy contig
    selection_type = check_decoy_selection(diversity_indices, decoy_contig)
    if selection_type == "DI":
        fancylog("Selecting a decoy contig based on the diversity indices...")
        used_decoy_contig = autoselect_decoy(
            diversity_indices,
            decoy_min_length,
            decoy_min_average_coverage,
            fancylog,
        )
        fancylog(
            f"Selected {used_decoy_contig} as the decoy contig.", prefix=""
        )
    else:
        used_decoy_contig = decoy_contig
        fancylog(f"The specified decoy contig is {used_decoy_contig}.")

    # verify that the decoy contig is actually contained in the FASTA file
    if used_decoy_contig not in contig_name2len:
        raise ParameterError(
            f"Selected decoy contig {used_decoy_contig} is not present in "
            f"{contigs}."
        )
    fancylog(
        (
            "Verified that this decoy contig is present in the BCF and FASTA "
            "files."
        ),
        prefix="",
    )

    # if someone chuckles at this, the project is successful
    # that's how it works
    fancylog("(Sorry for doubting you.)", prefix="")

    # Figure out the exact p or r values we'll iterate through -- these will
    # correspond to the columns in our output TSV of FDR estimation (i.e.
    # for each target contig, we'll produce this many FDR estimates).
    #
    # NOTE 1: For the time being, we just go through in increments of 1 (for
    # p, this is 0.01%; for r, this is just 1 read). We could support other
    # "step" values, but that's a lot of work for probably little benefit
    # (unless this ends up being a bottleneck, idk).
    #
    # NOTE 2: We do not produce an estimate for the exact high_p (or high_r)
    # value. The maximum threshold is that minus the step value (... which is
    # always 1, at least right now).
    fancylog(f"Determining range of value(s) of {thresh_type} to consider...")
    if thresh_type == "p":
        if high_p <= thresh_min:
            raise ParameterError(
                f"--high-p = {high_p:,} must be larger than the minimum p "
                f"used in the BCF ({thresh_min:,})."
            )
        thresh_high = high_p
    else:
        if high_r <= thresh_min:
            raise ParameterError(
                f"--high-r = {high_r:,} must be larger than the minimum r "
                f"used in the BCF ({thresh_min:,})."
            )
        thresh_high = high_r
    thresh_max = thresh_high - 1
    thresh_vals = range(thresh_min, thresh_high)
    fancylog(
        (
            f"We'll consider {len(thresh_vals):,} value(s) of {thresh_type}: "
            f"from {thresh_min:,} to {thresh_max:,}."
        ),
        prefix="",
    )

    # Extra clarification about indisputable mutations. I want to avoid taking
    # people by surprise with this.
    fancylog(
        (
            f"{thresh_type}-mutations for {thresh_type} \u2265 "
            f'{thresh_high:,} will be considered "indisputable."'
        ),
        prefix="",
    )
    fancylog(
        (
            'These "indisputable" mutations won\'t be included in the FDR '
            "estimation results."
        ),
        prefix="",
    )

    fancylog(
        f"Computing mutation rates for {used_decoy_contig} at these threshold "
        "values..."
    )
    # For each value in thresh_vals, compute the decoy genome's mutation rate.
    decoy_mut_rates = compute_decoy_contig_mut_rates(
        contigs,
        bcf_obj,
        thresh_type,
        thresh_vals,
        used_decoy_contig,
        decoy_context,
    )
    fancylog("Done.", prefix="")

    fancylog(
        "Computing mutation rates and FDR estimates for the "
        f"{len(contig_name2len) - 1:,} target contig(s)..."
    )

    # Create the header for the TSV files
    tsv_header = "Contig"
    for val in thresh_vals:
        tsv_header += f"\t{thresh_type}{val}"

    # ... and write it out. The header is the same for the FDR estimate file
    # and for the (# of mutations per megabase) file.
    for tsv_fp in (output_fdr_info, output_num_info):
        with open(tsv_fp, "w") as fdr_file:
            fdr_file.write(f"{tsv_header}\n")

    # Compute FDR estimates for each target contig.
    # This is analogous to the "Full" context-dependent option for the decoy
    # genome comptuation, so we can reuse a lot of code from that.
    target_contigs = bcf_contigs - {used_decoy_contig}

    # Open both files within a single "with" block:
    # https://stackoverflow.com/a/4617069 (requires Python 3.1, but... we're
    # already using f-strings [which require Python 3.6] so whatevs lol)
    with open(output_fdr_info, "a") as ff, open(output_num_info, "a") as nf:
        fdr_out = ""
        num_out = ""
        # Sort target contigs so that their rows in the FDR / num-per-Mb files
        # are in lexicographic order. Not really needed, but nice to have and
        # makes testing easier
        for tc_ct, target_contig in enumerate(sorted(target_contigs), 1):
            fdr_line, num_line = compute_target_contig_fdr_curve_info(
                bcf_obj,
                thresh_type,
                thresh_vals,
                target_contig,
                contig_name2len[target_contig],
                decoy_mut_rates,
            )

            fdr_out += fdr_line
            num_out += num_line

            # We'll "chunk" outputs -- we'll only perform a write operation
            # every (chunk_size) lines. This way, we don't need to perform
            # |TargetContigs| write operations (which I thought was a
            # bottleneck here, but after implementing this I don't notice a
            # speedup... well, whatever, now this code at least looks a bit
            # nicer).
            if tc_ct % chunk_size == 0:
                ff.write(fdr_out)
                nf.write(num_out)
                fdr_out = ""
                num_out = ""

        if fdr_out != "":
            ff.write(fdr_out)
            nf.write(num_out)

    fancylog("Done.", prefix="")

    # TODO: Verify that the decoy contig has a nonzero mutation rate?
    # If not, that's problematic, because
    # then we'd estimate the FDR as zero for every target contig. That
    # shouldn't happen most of the time, anyway. Maybe add an option to limit
    # auto-selection to just contigs with mutations? Hm, but that would be
    # annoying to implement, and users can always manually set a decoy contig.)


def get_optimal_threshold_values(fi, fdr):
    """Returns the optimal values of p or r for each contig's mutation calls.

    Parameters
    ----------
    fi: pd.DataFrame
        FDR information produced by "strainFlye fdr estimate". The indices
        (rows) correspond to contigs; the columns correspond to threshold
        values of p or r (sorted in ascending order from left to right). Cells
        indicate the estimated FDR for this contig at this threshold value (and
        may be NaN if no FDR estimate was defined for a given combination of
        (contig, threshold value)).

        This should have already been sanity-checked using
        load_and_sanity_check_fdr_file(), so we don't perform any validation on
        this DataFrame's structure here.

    fdr: float
        FDR at which (non-indisputable) mutation calls for each contig will be
        fixed.

    Returns
    -------
    pd.Series
        Has the same index as fi (so, this has one entry per contig). Each
        contig's entry will be the smallest threshold value (extracted
        from the column name -- i.e. "p15" will get converted to the int 15)
        at which this contig's FDR is less than or equal to the specified fdr.
        If no such threshold value exists (i.e. all estimated FDRs for this
        contig are greater than the specified fdr and/or are NaN), the contig's
        entry in this series will be None.
    """
    # For a given contig's FDR curve, we define four cases regarding how this
    # curve intersects with a fixed FDR (represented as a vertical line).
    # The goal is to, for each contig, identify the "optimal" value of p or r
    # that maximizes the number of mutations / mb we see while keeping the
    # estimated FDR below the fixed value.
    #
    # How do we identify this "optimal" value? By looking at the FDR curve.
    # We define four "cases" of how a FDR curve can look, relative to the fixed
    # FDR (which can be thought of as an infinite vertical line).
    #     |
    # Case 1. The curve crosses the line at exactly one point.
    #     |   This is easy to handle -- select the value of p or r just below
    #     |   the intersection for this contig.
    # ^  _|_____
    # | / |
    # |/  |
    # +---|------->
    #     |
    # Case 2. The curve crosses the line more than one point.
    #     |   (This is an unfortunate possibility, because we do not have the
    #     |   guarantee that FDR estimates increase monotonically with lower
    #     |   values of p or r.) In this case, we select the value of p or r
    #     |   just below the intersection for this contig with the highest
    #     |   # mutations / mb (in the plot below, at the top intersection).
    #     |
    #     |   Notably, this will always correspond to the lowest value of p or
    #     |   r that is <= the fixed FDR: lower values of p or r also "include"
    #     |   the p- or r-mutations called from higher values of p or r, so the
    #     |   lowest "passing" threshold value will also result in the highest
    #     |   number of mutations per megabase in the target contig.
    # ^  _|_____
    # | / |
    # |/  |
    # |\__|__
    # |  _|_/
    # | / |
    # |/  |
    # +---|------->
    #     |
    # Case 3. The curve crosses the line at zero points, and all estimated FDRs
    #     |   are LOWER than the fixed FDR. In this case, just select the
    #     |   lowest value of p or r used.
    #     |
    # ^ | |
    # | | |
    # |/  |
    # +---|------->
    #     |
    # Case 4. The curve crosses the line at zero points, and all estimated FDRs
    #     |   are HIGHER than the fixed FDR. In this case, our hands are tied;
    #     |   don't select any value of p or r. We can't call any
    #     |   non-indisputable mutations for this contig, at least not at this
    #     |   fixed FDR.
    #     |
    # ^   |    /
    # |   |   /
    # |   |  |
    # +---|------->

    # Convert the FDR information to a binary matrix:
    # True  means this FDR is <= the fixed FDR value
    # False means this FDR is >  the fixed FDR value (or is a NaN)
    #
    # Note that that the ~fi.isna() check (automatically rendering all NaNs as
    # False) isn't really required, since (per IEEE-754 standards, I think? --
    # see https://stackoverflow.com/a/1573715) running "NaN <= x" should yield
    # False for all x. However, I don't really want to rely on this implicit
    # condition, and I'll take making this 0.1 seconds slower if it means I
    # don't have to think about floating-point standards any more right now.
    lte_fdr = (~fi.isna()) & (fi <= fdr)

    # While we're at it, convert the columns of this DataFrame to their integer
    # values -- so, "p15" --> 15, for example. We should have already
    # sanity-checked this DataFrame's structure before calling this.
    lte_fdr.columns = fi.columns.str.slice(1).astype(int)

    def get_optimal_threshold(fdr_df_row):
        # We implicitly go through from lower to higher threshold values -- so
        # we'll select the lowest threshold value that is <= the fixed FDR.
        # This automatically accounts for Cases 1, 2, and 3 above.
        for ci, val in enumerate(fdr_df_row):
            if val:
                return fdr_df_row.index[ci]
        # If we make it here, then none of the threshold values were <= the
        # fixed FDR. So this is a "Case 4" situation -- return NaN.
        return np.nan

    # We can select each optimal threshold using DataFrame.apply(). This is
    # faster than naive looping through the DataFrame, but it could still
    # probably be sped up (although I'm not sure how exactly we could use
    # vectorization here). See
    # https://web.archive.org/web/20181106230656/https://engineering.upside.com/a-beginners-guide-to-optimizing-pandas-code-for-speed-c09ef2c6a4d6
    # for details.
    return lte_fdr.apply(get_optimal_threshold, axis=1)


def write_filtered_bcf(
    in_bcf_obj,
    out_bcf_fp,
    decoy_contig,
    optimal_thresh_vals,
    thresh_type,
    thresh_high,
    fancylog,
    verbose,
):
    """Writes out a filtered BCF based on optimal threshold values.

    The filtered BCF will include:

        1. "Indisputable" mutations (that pass thresh_high). We will include
           indisputable mutations from all contigs -- the target(s), and the
           decoy.

        2. Non-indisputable mutations (that do not pass thresh_high but pass
           the corresponding optimal_thresh_vals value for a target contig).
           We will only include these mutations from the target contigs, since
           we don't have an optimal threshold value generated for the decoy
           contig.

           If a target contig did not have an optimal threshold value given
           (i.e. its OTV is np.nan), then we will not include any
           non-indisputable mutations for this contig.

    Parameters
    ----------
    in_bcf_obj: pysam.VariantFile
        Object describing a BCF file produced by strainFlye's naive calling.

    out_bcf_fp: str
        Filepath to which a filtered version of in_bcf_obj will be written.

    decoy_contig: str
        Name of the decoy contig.

    optimal_thresh_vals: pd.Series
        Output of get_optimal_threshold_values(). The index corresponds to
        contig names; the values correspond to either the optimal value of p
        or r for a contig (expressed as a number), or np.nan (if no optimal
        value exists for this contig -- i.e. all of this contig's estimated
        FDRs were greater than the threshold or were undefined).

        Note that the dtype of this Series will probably be a float, because
        the presence of NaNs forces the dtype to be float. (This problem is
        documented, for example, at
        https://pandas.pydata.org/docs/user_guide/integer_na.html.) This isn't
        a problem, since I know that all of the numbers are integers (e.g.
        15.0) -- so we can call int() on them without worrying.

    thresh_type: str
        Either "p" or "r". We assume at this point that in_bcf_obj,
        optimal_thresh_vals, etc. all also use this thresh_type -- so please
        don't break this, ok?

    thresh_high: int
        The "high" (indisputable) value of p or r passed to
        "strainFlye fdr estimate".

    fancylog: function
        Logging function.

    verbose: bool
        If True, logs information about the number of mutations preserved for
        each contig.

    Returns
    -------
    None
    """
    # We will write out certain mutation "records" to this BCF.
    out_bcf_obj = pysam.VariantFile(out_bcf_fp, "wb", header=in_bcf_obj.header)

    # For each contig (considering both the decoy and target(s))...
    for contig in in_bcf_obj.header.contigs:

        num_written_muts = 0
        num_original_muts = 0

        thresh_used = thresh_high
        # We only call "indisputable" mutations for the decoy contig.
        if contig != decoy_contig:
            # Figure out the minimum threshold to use for this contig. If we
            # can only output indisputable mutations for this contig, then this
            # threshold will remain thresh_high; if we have a lower threshold
            # value defined, we can use that. (We can do this because mutations
            # that pass thresh_high will automatically also pass lower
            # thresholds.)
            otv = optimal_thresh_vals[contig]
            if not np.isnan(otv):
                thresh_used = otv

        for mut in in_bcf_obj.fetch(contig):

            alt_pos = mut.info.get("AAD")[0]
            cov_pos = mut.info.get("MDP")

            # We can figure out whether or not this mutation passes thresh_used
            # by using the calling functions from call_utils. Notably, we pass
            # 1 as min_alt_pos for call_p_mutation() because we don't care
            # about min_alt_pos at this point -- like, it applied to the
            # mutations the user screened for using "strainFlye call"
            is_mut = False
            if thresh_type == "p":
                is_mut = call_utils.call_p_mutation(
                    alt_pos, cov_pos, thresh_used, 1
                )
            else:
                is_mut = call_utils.call_r_mutation(alt_pos, thresh_used)

            if is_mut:
                out_bcf_obj.write(mut)
                num_written_muts += 1

            num_original_muts += 1

        if verbose:
            fancylog(
                (
                    f"Wrote out {num_written_muts:,} / {num_original_muts:,} "
                    f"mutations for {contig}."
                ),
                prefix="",
            )


def load_and_sanity_check_fdr_file(fdr_info, thresh_type):
    """Loads and sanity-checks a FDR TSV file.

    This sanity-checks the structure of the file -- it doesn't say anything
    about the contigs all matching up with a BCF file, for example. The main
    goal is just checking that this looks like the sort of TSV file that the
    estimate command would have generated.

    Parameters
    ----------
    fdr_info: str
        Filepath to a FDR TSV file.

    thresh_type: str
        Either "p" or "r", depending on which type of mutations were called.

    Returns
    -------
    fi: pd.DataFrame
        DataFrame containing the information stored in the TSV file. Indices
        (rows) correspond to contigs; columns correspond to threshold values
        of p or r (sorted in ascending order from left to right). Cells
        indicate the estimated FDR for this contig at this threshold value.

    Raises
    ------
    ParameterError
        If the TSV file seems malformed.

    Other errors
        Can be raised by pd.read_csv() if the file seems malformed.
        We don't attempt to catch these errors.
    """
    fi = pd.read_csv(fdr_info, sep="\t", index_col=0)
    # error prefix -- saves us some typing...
    ep = "Input FDR TSV file seems malformed"
    if fi.index.name != "Contig":
        raise ParameterError(f'{ep}: no "Contig" header?')
    if len(fi.index) < 1:
        raise ParameterError(f"{ep}: no contigs described?")
    if len(fi.columns) < 1:
        raise ParameterError(f"{ep}: no threshold values described?")

    prev_col_val = None
    for col in fi.columns:
        if col[0] != thresh_type:
            raise ParameterError(
                f"{ep}: columns should start with {thresh_type}."
            )
        col_val = int(col[1:])
        if prev_col_val is not None:
            if col_val <= prev_col_val:
                raise ParameterError(
                    f"{ep}: values of {thresh_type} should increase from left "
                    "to right."
                )
            # We could eventually support different step sizes, but not yet.
            # Actually I think this might be overzealous -- I don't think we do
            # anything in "fix" that requires known step sizes -- but whatever,
            # I wanna be careful.
            if col_val != prev_col_val + 1:
                raise ParameterError(
                    f"{ep}: values of {thresh_type} should only increase in "
                    "steps of 1."
                )
        prev_col_val = col_val

        col_fdrs = fi[col]
        # Check that this column is numeric (NaNs are ok).
        # See https://stackoverflow.com/a/45568283.
        if not pd.api.types.is_numeric_dtype(col_fdrs):
            raise ParameterError(
                f"{ep}: Column {col} doesn't seem to be numeric?"
            )

        # At this point, we've already screened for non-numeric dtypes, so we
        # should be able to use < 0 without problems
        if (col_fdrs < 0).any():
            raise ParameterError(
                f"{ep}: Column {col} contains negative FDR estimates?"
            )

    return fi


def log_optimal_threshold_value_stats(
    optimal_thresh_vals, thresh_type, thresh_min, thresh_max, fdr, fancylog
):
    """Logs information about the optimal threshold values.

    Selecting these values is arguably the main part of how "strainFlye fdr
    fix" works, so I wanted to be as clear as possible -- hence this function
    existing.

    Parameters
    ----------
    optimal_thresh_vals: pd.Series
        Output of get_optimal_threshold_values(). The index corresponds to
        contig names; the values correspond to either the optimal value of p
        or r for a contig (expressed as a number), or np.nan (if no optimal
        value exists for this contig -- i.e. all of this contig's estimated
        FDRs were greater than the threshold or were undefined).

        Note that the dtype of this Series will probably be a float, because
        the presence of NaNs forces the dtype to be float. (This problem is
        documented, for example, at
        https://pandas.pydata.org/docs/user_guide/integer_na.html.) This isn't
        a problem, since I know that all of the numbers are integers (e.g.
        15.0) -- so we can call int() on them without worrying.

    thresh_type: str
        Either "p" or "r".

    thresh_min: int
        Minimum value of p or r used in "strainFlye call".

    thresh_max: int
        The maximum value of p or r: this is equal to the "high" (indisputable)
        value of p or r passed to "strainFlye fdr estimate" minus 1. Note that
        this can technically be equal to thresh_min in the silly case where
        --high-p = --min-p + 1 (same goes for r), although that really
        shouldn't happen in practice.

    fdr: float
        FDR at which (non-indisputable) mutation calls for each contig will be
        fixed.

    fancylog: function
        Logging function.

    Returns
    -------
    None
    """
    # A contig can have a N/A optimal threshold value if its entire FDR curve
    # was > the fixed FDR (see Case 4 in get_optimal_threshold_values()).
    nonna = optimal_thresh_vals[~optimal_thresh_vals.isna()]
    num_nonna = len(nonna)
    num_targets = len(optimal_thresh_vals)

    if num_nonna == 0:
        # This should not happen except for really weird cases
        fancylog(
            (
                f"Warning: No values of {thresh_type} resulted in estimated "
                f"FDRs \u2264 the fixed FDR, for all {num_targets:,} contigs."
            ),
            prefix="",
        )
    else:
        fancylog(
            (
                f"For {num_nonna:,} / {num_targets:,} contigs, there exist "
                f"values of {thresh_type} (at least, considering the range "
                f"from {thresh_type} = {thresh_min} to {thresh_type} = "
                f"{thresh_max}) that yield estimated FDRs \u2264 {fdr}%."
            ),
            prefix="",
        )
        # NOTE: Could roll these into a single loop to speed this up, but this
        # almost certainly won't be a bottleneck
        # Also see above re: int() calls -- all of the numbers here should ints
        # that just end with ".0", so we're not actually losing any information
        min_contig = nonna.idxmin()
        min_tv = int(nonna[min_contig])
        max_contig = nonna.idxmax()
        max_tv = int(nonna[max_contig])
        mean_tv = mean(nonna)
        fancylog(
            (
                f"These values range from {thresh_type} = {min_tv:,} "
                f"({min_contig}) to {thresh_type} = {max_tv:,} ({max_contig})."
            ),
            prefix="",
        )
        fancylog(
            f"The mean of these values is {thresh_type} = {mean_tv:,.2f}.",
            prefix="",
        )


def run_fix(bcf, fdr_info, fdr, output_bcf, fancylog, verbose):
    """Performs FDR fixing.

    Parameters
    ----------
    bcf: str
        Filepath to a BCF file generated by one of strainFlye call's
        subcommands.

    fdr_info: str
        Filepath to a TSV file describing estimated FDRs for the target
        contigs.

    fdr: int
        False Discovery Rate (FDR) to fix mutation calls at. Scaled up by 100,
        like values of p are elsewhere -- so fdr = 100 indicates an FDR of 1%.

    output_bcf: str
        Filepath to which we will write an (indexed) BCF file. This will
        contain a subset of the mutations described in the input BCF file.

    fancylog: function
        Logging function.

    verbose: bool
        If True, display information about each contig while writing the
        filtered BCF. At least on the SheepGut dataset, this step takes the
        longest, and it can be useful to allow verbosity there.

    Returns
    -------
    None

    Raises
    ------
    ParameterError
        - If the set of contigs in the FDR TSV file is not a subset of those
          in the BCF file.
        - If there is not exactly one contig that is present in the BCF file
          but not in the FDR TSV file.

    - parse_sf_bcf() can also raise various errors if the input BCF is
      malformed.
    """
    fancylog("Loading and checking BCF and TSV files...")
    # Like in run_estimate(): Load the BCF file and figure out what contigs it
    # describes
    bcf_obj, thresh_type, thresh_min = parse_sf_bcf(bcf)
    bcf_contigs = set(bcf_obj.header.contigs)

    # Load the estimated FDR file.
    fi = load_and_sanity_check_fdr_file(fdr_info, thresh_type)
    tsv_desc = "the FDR TSV file"
    # Ensure that the contigs described in these TSV files (tsv_contigs)
    # are all described in the BCF file. We don't check for an exact match,
    # because the decoy contig will be missing.
    tsv_contigs = set(fi.index)
    misc_utils.verify_contig_subset(
        tsv_contigs, bcf_contigs, tsv_desc, "the BCF file"
    )
    absent_contigs = bcf_contigs - tsv_contigs
    if len(absent_contigs) != 1:
        # Fun thing about this error message: we can say "contigs" because this
        # error fires if and only if len(absent_contigs) is not 1 :D
        raise ParameterError(
            "Exactly one contig in the BCF file (the decoy contig) should be "
            f"missing from {tsv_desc}. However, {len(absent_contigs):,} "
            "contigs are missing!"
        )

    # If we've made it here, we know there is exactly one contig in the BCF but
    # not in the FDR TSV file. This must be the decoy contig.
    decoy_contig = absent_contigs.pop()
    fancylog(
        f"Looks good so far; decoy contig seems to be {decoy_contig}.",
        prefix="",
    )

    # Figure out the "indisputable" mutation cutoff.
    # At this point, we already know that the FDR info file is structured as
    # expected (e.g. columns are all formatted like "p15", "p16", etc.), so we
    # can slice off the first character from the last column without worrying
    # about the file being structured incorrectly.
    thresh_max = int(fi.columns[-1][1:])
    thresh_high = thresh_max + 1
    fancylog(
        (
            'Looks like the cutoff for "indisputable" mutations was '
            f"{thresh_type} = {thresh_high:,}."
        ),
        prefix="",
    )
    fancylog(
        (
            "All mutations passing this cutoff will be included in the "
            "output BCF file."
        ),
        prefix="",
    )

    fancylog(
        "Based on the FDR information, finding optimal values of "
        f"{thresh_type} for each contig..."
    )
    # We can now begin in earnest. Figure out the "optimal" value of p or r for
    # for each contig, based on the estimated FDR information.
    optimal_thresh_vals = get_optimal_threshold_values(fi, fdr)
    fancylog("Done.", prefix="")
    log_optimal_threshold_value_stats(
        optimal_thresh_vals, thresh_type, thresh_min, thresh_max, fdr, fancylog
    )

    # VCF / BCF headers aren't (currently) editable using pysam
    # (https://github.com/pysam-developers/pysam/issues/668), but we can
    # fortunately use "bcftools annotate" to do this the long(ish) way -- write
    # the "first" BCF file to a temporary file, then annotate it, and output
    # the annotated BCF to the final "output BCF" location. (Annotation doesn't
    # seem to be doable "in place" using bcftools.)
    with tempfile.NamedTemporaryFile(
        mode="w+b"
    ) as temp_bcf_file, tempfile.NamedTemporaryFile(mode="w+") as header_file:
        fancylog(
            "Writing a filtered BCF file (to a temporary location, for now) "
            "including both "
            "(1) indisputable mutations from all contigs and "
            "(2) non-indisputable mutations from the target contigs that "
            f"result in a FDR \u2264 {fdr}%..."
        )
        # Filter mutations for each contig to those that pass these thresholds
        # (in addition to indisputable mutations that pass thresh_high).
        write_filtered_bcf(
            bcf_obj,
            temp_bcf_file.name,
            decoy_contig,
            optimal_thresh_vals,
            thresh_type,
            thresh_high,
            fancylog,
            verbose,
        )
        fancylog("Done.", prefix="")

        fancylog(
            "Updating the filtered BCF file's header to indicate that FDR "
            "fixing was done..."
        )
        # We don't have information about the decoy context (Full, CP2, ...)
        # but that's not a big deal. the main thing is just making it clear
        # that this file has been updated from the naive calls
        new_header_line = (
            f"##FILTER=<ID=strainFlye_fdr_fix_fdr_{fdr}_decoy_{decoy_contig}_"
            f'high_{thresh_type}_{thresh_high}, description="Mutations '
            'filtered to a fixed FDR">'
        )
        with open(header_file.name, "w") as hf:
            hf.write(new_header_line)
        # Update the header in the temp BCF file, and -- while this is
        # happening -- write out the updated BCF file to the output location
        subprocess.run(
            [
                "bcftools",
                "annotate",
                "-h",
                header_file.name,
                "-O",
                "b",
                "-o",
                output_bcf,
                temp_bcf_file.name,
            ],
            check=True,
        )
        fancylog("Done.", prefix="")

        # just gotta index the BCF, then we're done!
        bcf_utils.index_bcf(output_bcf, fancylog)
