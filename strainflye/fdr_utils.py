# Utilities for strainFlye fdr.


import os
import tempfile
import subprocess
import pysam
import skbio
import numpy as np
import pandas as pd
from math import floor
from statistics import mean
from collections import defaultdict
from strainflye import fasta_utils, call_utils, misc_utils, bcf_utils, config
from strainflye.bcf_utils import parse_sf_bcf
from .errors import ParameterError, SequencingDataError, WeirdError


def check_decoy_selection(diversity_indices, decoy_contig):
    """Checks that only one of (diversity index file, decoy contig) is given.

    Parameters
    ----------
    diversity_indices: str or None
        If a str, this should be a filepath to a TSV file representing
        diversity index info.

    decoy_contig: str or None
        If a str, this should be the name of a contig described in the
        BCF file.

    Returns
    -------
    selection_type: str
        Will be "DI" if only diversity_indices is not None, and will be "DC" if
        only decoy_contig is not None.

    Raises
    ------
    ParameterError
        - If diversity_indices and decoy_contig are both not None
        - If diversity_indices and decoy_contig are both None
    """
    di = diversity_indices is not None
    dc = decoy_contig is not None

    if di:
        if dc:
            raise ParameterError(
                "Both the diversity indices file and a decoy contig are "
                "specified. These options are mutually exclusive."
            )
        else:
            return "DI"
    else:
        if dc:
            return "DC"
        else:
            raise ParameterError(
                "Either the diversity indices file or a decoy contig must be "
                "specified."
            )


def normalize_series(in_series):
    """Converts a series to values in the range [0, 1].

    Parameters
    ----------
    in_series: pd.Series
        We assume that this does not contain any nan values.

    Returns
    -------
    None or pd.Series
        If the minimum and maximum of in_series are identical, this will just
        return None. (In this case, we can't scale values, because the
        denominator we use when converting a value (max - min) is zero.)

        If the minimum and maximum are not identical (which should usually
        be the case with diversity indices, hopefully...) then this will return
        a pd.Series with the same index as in_series, but with each entry
        scaled to within the range [0, 1] (such that the min value in in_series
        is set to 0, the max is set to 1, and everything else is in between).
    """
    # Small TODO: in theory, it'd be faster to combine the
    # computation of min and max into a single pass over the values
    # (see e.g. https://stackoverflow.com/q/12200580) but this
    # probably won't be a performance bottleneck so I'm not gonna
    # bother for now
    min_val = min(in_series)
    max_val = max(in_series)
    if min_val == max_val:
        return None
    else:
        # Use pandas' vectorization to apply linear interpolation
        # across all diversity indices in this Series
        return (in_series - min_val) / (max_val - min_val)


def autoselect_decoy(diversity_indices, min_len, min_avg_cov, fancylog):
    """Attempts to select a good decoy contig based on diversity index data.

    There are lots of ways to implement this, so here we just stick with
    something simple that combines the diversity index information from
    multiple thresholds:

    1. Filter to all contigs whose lengths and average coverages meet the
       specified thresholds. The number of "passing" contigs is C. If C = 1,
       select this contig; if C = 0, raise an error.

    2. For each of the D diversity index columns provided in the file (where at
       least two contigs have defined diversity indices), compute the minimum
       and maximum diversity index in this column. Assign each contig a score
       for this column in [0, 1] using linear interpolation: the contig with
       the lowest diversity index gets a score of 0, the contig with the
       highest gets a score of 1, and everything else is scaled in between.
       If a contig has an undefined diversity index in such a column, set its
       score for this column to 1.

    3. For each of the C contigs, sum scores across all of the D diversity
       index columns. Select the contig with the lowest score sum. Break ties
       arbitrarily.

    Parameters
    ----------
    diversity_indices: str
        Filepath to a TSV file containing diversity index info, generated by
        one of strainFlye call's subcommands. In addition to diversity index
        values, this also includes length and average coverage information
        for each contig in the file (this will help a lot, since computing
        these values if we don't already have them is time-consuming or at
        the very least annoying).

    min_len: int
        In order for a contig to be selected as the decoy, its length must be
        at least this.

    min_avg_cov: float
        In order for a contig to be selected as the decoy, its average coverage
        must be at least this.

    fancylog: function
        Logging function.

    Returns
    -------
    decoy_contig: str
        The name of the decoy contig we find.

    Raises
    ------
    ParameterError
        If the diversity index file:
        - Describes < 2 contigs (should have already been caught during align,
          but you never know)
        - Doesn't have Length or AverageCoverage columns

    SequencingDataError
        - If none of the contigs in the diversity index file pass the length
          and average coverage thresholds.
        - If none of the diversity index columns has at least two "passing"
          contigs with defined and distinct diversity indices in this column.
    """
    # We require that the diversity indices file describes at least two
    # contigs, so that -- once we select one contig as a decoy -- we still have
    # at least one target contig left!
    di = misc_utils.load_and_sanity_check_diversity_indices(
        diversity_indices, min_num_contigs=2
    )

    # Filter to contigs that pass both the length and coverage thresholds.
    # https://stackoverflow.com/a/13616382
    passing_di = di[
        (di["Length"] >= min_len) & (di["AverageCoverage"] >= min_avg_cov)
    ]
    passing_contigs = passing_di.index
    num_passing_contigs = len(passing_contigs)
    # it isn't clear how much precision min_avg_cov has, so we don't impose any
    # limit on how many digits it goes out to when printing it out. let's let
    # python handle this one
    check_str = (
        f"the min length \u2265 {min_len:,} and min average cov \u2265 "
        f"{min_avg_cov:,}x checks"
    )
    if num_passing_contigs == 0:
        raise SequencingDataError(f"No contigs pass {check_str}.")
    elif num_passing_contigs == 1:
        # Arguably, we could raise an error here -- but we know at this point
        # that there are >= 2 contigs in the file (and this is just the only
        # one that passes the length and coverage thresholds). So we may as
        # well select this contig, albeit after giving a warning.
        fancylog(
            f"Warning: Only one contig passes {check_str}. Selecting it.",
            prefix="",
        )
        return passing_contigs[0]

    # Actually start scoring contigs based on their diversity indices.
    contig2score = defaultdict(int)
    # Diversity index columns where there are at least two "passing" contigs
    # that have defined (non-NA) diversity indices
    good_di_cols = []
    for di_col in passing_di.columns:

        # ignore non-diversity-index columns
        if di_col.startswith(config.DI_PREF):
            di_vals = passing_di[di_col]

            # Ignore diversity index columns where less than two contigs have
            # defined diversity indices, since these don't mean much for our
            # score computation (at least as currently defined)
            finite_di_vals = di_vals[~di_vals.isna()]
            if len(finite_di_vals.index) >= 2:

                scores = normalize_series(finite_di_vals)
                # normalize_series() will return None if the min and max value
                # in finite_di_vals are identical. In this case, we can't
                # generate meaningful scores, so we just move on.
                if scores is not None:
                    good_di_cols.append(di_col)

                    # Update scores.
                    for passing_contig in passing_di.index:
                        if passing_contig in scores:
                            contig2score[passing_contig] += scores[
                                passing_contig
                            ]
                        else:
                            # Penalize this contig for not having a defined
                            # diversity index in this column: give it the max
                            # possible score
                            contig2score[passing_contig] += 1

    if len(good_di_cols) == 0:
        raise SequencingDataError(
            "No diversity index column has at least two contigs that (1) pass "
            f"{check_str} and (2) have defined and distinct diversity indices "
            "in this column."
        )
    # Find the passing contig with the lowest total score:
    # https://stackoverflow.com/a/3282904
    lowest_score_contig = min(passing_contigs, key=contig2score.get)
    return lowest_score_contig


def verify_thresh_vals_good(thresh_vals):
    if thresh_vals.step != 1:
        raise ParameterError("thresh_vals must use a step size of 1.")
    if len(thresh_vals) <= 0:
        raise ParameterError("thresh_vals must have a positive length.")
    if thresh_vals.start <= 0 or thresh_vals.stop <= 0:
        raise ParameterError("thresh_vals' start and stop must be positive.")


def compute_number_of_mutations_in_contig(
    bcf_obj, thresh_type, thresh_vals, contig, pos_to_consider=None
):
    """Counts mutations at certain p or r thresholds in a contig.

    This function is designed to be useful for either decoy or target contigs.

    We perform some sanity checking on thresh_vals, but we'll assume that
    the other parameters are well-formed (e.g. thresh_type is either "p" or
    "r", contig is in bcf_obj, etc.)

    Parameters
    ----------
    bcf_obj: pysam.VariantFile
        Object describing a BCF file produced by strainFlye's naive calling.

    thresh_type: str
        Either "p" or "r", depending on which type of mutations were called in
        bcf_obj.

    thresh_vals: range
        Range of values of p or r (depending on thresh_type) at which to
        count mutations in this contig. Must use a step size of 1.
        If a mutation is a mutation for a value of p or r larger than the
        maximum p or r value here (i.e. it's "indisputable"), it will not be
        counted.

    contig: str
        Name of a contig for which mutation rates will be computed.

    pos_to_consider: set or None
        If this is None, then consider mutations at *all* positions in the
        contig.

        If this is a set, then we assume that this set describes 1-indexed
        positions in this contig; we will then only count mutations occurring
        in positions given in this set.

    Returns
    -------
    num_muts: list of int
        List with the same length as thresh_vals. The i-th value in this list
        describes the number of called mutations for the i-th threshold in
        thresh_vals.

    Raises
    ------
    ParameterError
        If thresh_vals doesn't use a step size of 1.
        If len(thresh_vals) is zero.
        If either the stop or start of thresh_vals are zero or below.

        (None of these should happen in practice, but you never know...)
    """
    verify_thresh_vals_good(thresh_vals)

    # For each threshold value, keep track of how many mutations we've seen at
    # this threshold.
    num_muts = [0] * len(thresh_vals)

    # We can just infer the "indisputable" mutation value from thresh_vals as
    # the value just above the max thresh_vals entry
    high_val = thresh_vals[-1] + 1
    min_val = thresh_vals[0]

    for mut in bcf_obj.fetch(contig):

        # only count this mutation if (1) we are considering all positions, or
        # (2) we are only considering some positions and this is one of them.
        # (this abuses short circuiting)
        if pos_to_consider is None or mut.pos in pos_to_consider:
            # AAD is technically a tuple since it's defined once for every alt
            # allele, but r/n strainflye call only produces max one alt allele
            # per mutation. So it's a tuple with 1 element (at least for now).
            alt_pos = mut.info.get("AAD")[0]
            cov_pos = mut.info.get("MDP")

            if thresh_type == "p":
                max_passing_val = floor((10000 * alt_pos) / cov_pos)
            else:
                max_passing_val = alt_pos

            # Don't count "indisputable" mutations towards mutation rates
            if max_passing_val >= high_val:
                continue

            # NOTE: This is already more optimized than the analysis
            # notebooks, but I think it could still be made faster. Maybe just
            # increment a single value (corresponding to the max passing p/r),
            # and then do everything at the end after seeing all mutations in
            # one pass? Doesn't seem like a huge bottleneck tho.
            num_vals_to_update = max_passing_val - min_val + 1
            for i in range(num_vals_to_update):
                num_muts[i] += 1
    return num_muts


def compute_full_decoy_contig_mut_rates(
    bcf_obj, thresh_type, thresh_vals, contig, contig_len
):
    """Computes mutation rates for the entirety of a decoy contig.

    This is designed for the "Full" option, in which we consider every position
    in the contig as part of the decoy.

    Parameters
    ----------
    bcf_obj: pysam.VariantFile
        Object describing a BCF file produced by strainFlye's naive calling.

    thresh_type: str
        Either "p" or "r", depending on which type of mutations were called in
        bcf_obj.

    thresh_vals: range
        Range of values of p or r (depending on thresh_type) at which to
        compute mutation rates for this contig. Must use a step size of 1.
        If a mutation is a mutation for a value of p or r larger than the
        maximum p or r value here (i.e. it's "indisputable"), it will not be
        included in the mutation rate computation.

    contig: str
        Decoy contig name.

    contig_len: int
        Decoy contig sequence length.

    Returns
    -------
    mut_rates: list of float
        Mutation rates for each threshold value in thresh_vals.
    """
    num_muts = compute_number_of_mutations_in_contig(
        bcf_obj, thresh_type, thresh_vals, contig
    )
    denominator = 3 * contig_len
    return [n / denominator for n in num_muts]


def complain_about_cps(gn, gs, cp):
    """Raises an error about codon position while iterating through a gene.

    Mostly intended as a fail-safe to catch weird errors. See
    get_single_gene_cp2_positions() for context on why this function is even
    useful.

    Parameters
    ----------
    gn: int
        ID number for a gene.

    gs: str
        Gene strand. Should be "+" or "-", but we don't do any sanity checking
        here.

    cp: int
        Codon position. In practice, this function should be called if this
        gets "off" somehow (i.e. it isn't 1, 2, or 3).

    Raises
    ------
    WeirdError
    """
    raise WeirdError(
        f"Codon position got out of whack: gene {gn:,}, strand {gs}, CP {cp}"
    )


def get_single_gene_cp2_positions(genes_df):
    """Returns a set of all positions located in exactly one gene and in CP2.

    Parameters
    ----------
    genes_df: pd.DataFrame
        Describes predicted genes in a contig. See parse_sco().

    Returns
    -------
    single_gene_cp2_positions: set
        Set of all positions located in exactly one gene, and located in CP2
        (in the second codon position) of their parent gene.

    Raises
    ------
    WeirdError
        If something goes wrong during iteration; this should never happen, but
        you never know.

    Notes
    -----
    This is pretty inefficient. I doubt it will be a bottleneck, since this
    should only be called once per run of "fdr estimate", but if you need to
    speed this up a good first step might be moving from itertuples() to
    something like .apply() / vectorization.
    """
    # records all positions in CP2 of at least one gene
    cp2_pos = set()
    # records all positions in at least one gene that we have seen thus far
    pos_seen_in_other_genes = set()
    # records all positions known to be in multiple genes
    multi_gene_pos = set()

    # PERF: yeah yeah yeah use something faster than itertuples
    for gene in genes_df.itertuples():
        if gene.Strand == "+":
            cp = 1
        else:
            cp = 3
        for pos in range(gene.LeftEnd, gene.RightEnd + 1):
            # if we've already seen this position in another gene, then this
            # position is contained in multiple genes. So: we won't include it
            # as part of the decoy.
            if pos in pos_seen_in_other_genes:
                multi_gene_pos.add(pos)

            # Keep track of ALL CP2 positions we see, even those in multiple
            # genes. (We could try to only do this if the above check is False,
            # but that wouldn't prevent against the case where we see a
            # position and then later find out it's in another gene. Simpler to
            # do things this way; not gonna optimize this too much right now.)
            if cp == 2:
                cp2_pos.add(pos)

            # Make it clear that we've seen this position in at least one gene.
            pos_seen_in_other_genes.add(pos)

            # There's probably a more elegant way to do this, but this seems
            # like the clearest (and least prone to off by one errors) way.
            #
            # ...BREAKING NEWS: local grad student "too stupid to use
            # basic modulo arithmetic"; bystanders SHOCKED at this SUSSY
            # IMPOSTOR BEHAVIOR; more at 11
            if gene.Strand == "+":
                # 123123123...
                if cp == 1 or cp == 2:
                    cp += 1
                elif cp == 3:
                    cp = 1
                else:
                    complain_about_cps(gene.Index, gene.Strand, cp)
            else:
                # 321321321...
                if cp == 3 or cp == 2:
                    cp -= 1
                elif cp == 1:
                    cp = 3
                else:
                    complain_about_cps(gene.Index, gene.Strand, cp)

    return cp2_pos - multi_gene_pos


def compute_cp2_decoy_contig_mut_rates(
    bcf_obj, thresh_type, thresh_vals, contig, contig_genes_df
):
    """Computes mutation rates for just CP2 positions in a decoy contig.

    This is designed for the "CP2" option, in which we consider each position
    in the second codon position of a gene (ignoring positions located in
    multiple genes) as part of the decoy.

    Parameters
    ----------
    bcf_obj: pysam.VariantFile
        Object describing a BCF file produced by strainFlye's naive calling.

    thresh_type: str
        Either "p" or "r", depending on bcf_obj.

    thresh_vals: range
        Range of values of p or r (depending on thresh_type) at which to
        compute mutation rates for this contig. Must use a step size of 1.
        If a mutation is a mutation for a value of p or r larger than the
        maximum p or r value here (i.e. it's "indisputable"), it will not be
        included in the mutation rate computation.

    contig: str
        Decoy contig name.

    contig_genes_df: pd.DataFrame
        Describes predicted genes in the contig; see parse_sco() for details.

    Returns
    -------
    mut_rates: list of float
        Mutation rates for each threshold value in thresh_vals.
    """
    single_gene_cp2_pos = get_single_gene_cp2_positions(contig_genes_df)
    num_muts = compute_number_of_mutations_in_contig(
        bcf_obj,
        thresh_type,
        thresh_vals,
        contig,
        pos_to_consider=single_gene_cp2_pos,
    )
    denominator = 3 * len(single_gene_cp2_pos)
    return [n / denominator for n in num_muts]


def compute_target_contig_fdr_curve_info(
    bcf_obj,
    thresh_type,
    thresh_vals,
    target_contig,
    target_contig_len,
    ctx2mr,
):
    """Computes FDR curve information for a given target contig.

    The intent is to create output that can be dropped straight into a TSV
    file, without too much work on the part of the caller.

    Parameters
    ----------
    bcf_obj: pysam.VariantFile
        Object describing a BCF file produced by strainFlye's naive calling.

    thresh_type: str
        Either "p" or "r", depending on which type of mutations were called in
        bcf_obj.

    thresh_vals: range
        Range of values of p or r (depending on thresh_type) at which to
        count mutations in this contig. Must use a step size of 1.
        If a mutation is a mutation for a value of p or r larger than the
        maximum p or r value here (i.e. it's "indisputable"), it will not be
        included in the FDR curve information computation.

    target_contig: str
        Name of the target contig.

    target_contig_len: int
        Target contig sequence length.

    ctx2mr: dict
        Maps values in decoy_contexts to a list of mutation rates computed for
        the decoy contig using this context. See
        compute_decoy_contig_mut_rates() for details.

    Returns
    -------
    (ctx2fdr_lines, num_line): (dict, str)
        Each value in ctx2fdr_lines (and num_line itself) are tab-separated
        lines (suitable for adding to a TSV file where the first column is the
        target contig name and there are len(thresh_vals) additional columns).

        The first entry in each line in ctx2fdr_lines, and the first entry in
        num_line, is the target contig name. The remaining entries in each
        line in ctx2fdr_lines describe the estimated FDR for this target contig
        for the decoy mutation rate at this context at each threshold value
        (aka the x-axis on a FDR curve, as drawn in the paper). The
        remaining entries in num_line describe the number of mutations per
        megabase for this target contig at each threshold value (the y-axis
        on a FDR curve, as drawn in the paper).
    """
    # Get the number of mutations in this target contig at each of the
    # threshold values
    num_muts = compute_number_of_mutations_in_contig(
        bcf_obj, thresh_type, thresh_vals, target_contig
    )

    # it's a long story. see docs for compute_num_mutations_per_mb() in
    # https://github.com/fedarko/sheepgut/blob/main/notebooks/DemonstratingTargetDecoyApproach.ipynb
    numpermb_coeff = 1000000 / target_contig_len
    denominator = 3 * target_contig_len
    # The 100 is because we convert FDRs from [0, 1] to percentages
    # (technically, FDRs can exceed 100% if the decoy mutation rate > the
    # target mutation rate, but that shouldn't happen too often)
    fdr_coeff = 100 * denominator

    ctx2fdr_lines = {ctx: target_contig for ctx in ctx2mr}
    num_line = target_contig

    # TODO: Maybe speed this up by first creating a pandas DataFrame of all
    # contigs and their num_muts, then using vectorization or apply() to do
    # this stuff on all contigs at once? But this runs in ~2 minutes on the
    # entire SheepGut dataset (edit: or at least it did before i added
    # context-dependent stuff back in), so it's not a substantial bottleneck.

    for i, n in enumerate(num_muts):
        if n == 0:
            # The FDR is undefined if the target's mutation rate is zero
            for ctx in ctx2mr:
                ctx2fdr_lines[ctx] += "\tNA"
            # And, of course, there are 0 mutations / Mb
            num_line += "\t0"

        else:
            # the (fdr coeff / n) thing is the same for all contexts at this
            # threshold, so we can pre-compute it and then reuse it for all the
            # different contexts for this target contig for this threshold
            # value. might be overkill but whatevs.
            fdr_cell_coeff = fdr_coeff / n
            for ctx in ctx2mr:
                ctx2fdr_lines[ctx] += "\t" + str(
                    fdr_cell_coeff * ctx2mr[ctx][i]
                )

            num_line += f"\t{numpermb_coeff * n}"

    # "cross your t's, dot your i's, and add your newlines" -- hillary clinton
    for ctx in ctx2fdr_lines:
        ctx2fdr_lines[ctx] += "\n"
    num_line += "\n"

    return ctx2fdr_lines, num_line


def parse_sco(sco_fp):
    """Returns a DataFrame representing a SCO file describing gene predictions.

    This function is mostly intended for internal use at the moment, since I
    expect that -- in the case of e.g. the hotspot features stuff -- most
    people will have GFF3 files. (But if a lot of people have SCO files, then
    I guess we could modify that side of things to accept these as well.)

    Parameters
    ----------
    sco_fp: str
        Filepath to a SCO ("Simple Coordinate Output") file.

    Returns
    -------
    df: pd.DataFrame
        Describes genes in the SCO file. Rows are indexed based on the
        1-indexed gene number in the SCO file; there are four columns,
        "LeftEnd", "RightEnd", "Length", and "Strand". LeftEnd and RightEnd
        are 1-indexed and inclusive.

    Raises
    ------
    WeirdError
        If various things look wrong with the SCO file. I raise this instead
        of a ParameterError, because problems here indicate that (probably)
        something is up with Prodigal, rather than with how the user is running
        strainFlye. (Although I guess we could have multiple sources of
        error...)

    ValueError
        Implicitly raised if things we expect to be integers in the SCO file
        (e.g. gene coordinates) are not.

    References
    ----------

    +---------------------------------+
    | What does a SCO file look like? |
    +---------------------------------+

    Prodigal's documentation
    (https://github.com/hyattpd/Prodigal/wiki/understanding-the-prodigal-output)
    points to http://tico.gobics.de/ioexamples.jsp regarding details of the SCO
    file format. As of writing (September 4, 2022), this link is dead.
    But! We can figure out what it said with the wayback machine:
    https://web.archive.org/web/20060717224112/http://tico.gobics.de/ioexamples.jsp

    Copying from there:

        The Simple Coord format just gives the coordinates of the predicted
        ORFs with an id and the orientation. The input may also contain a
        score and a label as in the Simple Coord output of TiCo.

        >id_left_right_strand[_score][#]

        Example:

        >2_337_2799_+
        >3_2801_3733_+
        >5_3734_5020_+
        >6_5088_5237_+
        >8_5310_5720_-
        >10_5683_6459_-
        >12_6529_7959_-
        >14_8175_9191_+
        >15_9303_9893_+
        >17_9928_10494_-
        >19_10643_11356_-

    Here, we ignore the presence of a score / label, and just focus on
    processing the first four fields.

    +--------------------------------------+
    | ok but like why does this code exist |
    +--------------------------------------+

    I initially wrote this to mimic the way LST files from GeneMark
    can be parsed with Pandas (...with some effort). Hence the reuse of
    "LeftEnd", "RightEnd", etc. I figured it was simpler to adapt this code
    then it was to re-write stuff to work with GFF3 files.

    Also, the initial version of this code comes from the SheepGut repo:
    https://github.com/fedarko/sheepgut/blob/main/notebooks/parse_sco.py
    """
    genes = {}
    with open(sco_fp, "r") as f:
        for line in f:
            # Ignore comments
            if not line.startswith("#"):
                if line.startswith(">"):
                    # Ignore any "parts" after the strand -- we don't care
                    # about scores/labels in SCO files (see refs above)
                    parts = line[1:].strip().split("_")

                    # ValueErrors will be raised if any of these parts don't
                    # look like a number
                    gene_num = int(parts[0])
                    left_end = int(parts[1])
                    rght_end = int(parts[2])

                    strand = parts[3]
                    if strand != "+" and strand != "-":
                        raise WeirdError(
                            f"Unrecognized strand in SCO: {strand}"
                        )

                    # left end should always be on the left side, regardless of
                    # strand
                    if left_end >= rght_end:
                        raise WeirdError(
                            f"Gene {gene_num:,} in SCO has left end of "
                            f"{left_end:,}, which is not < the right end of "
                            f"{rght_end:,}."
                        )

                    length = rght_end - left_end + 1
                    if length % 3 != 0:
                        raise WeirdError(
                            f"Gene {gene_num:,} in SCO is {length:,} bp long; "
                            "lengths must be divisible by 3."
                        )

                    genes[gene_num] = [left_end, rght_end, length, strand]
                else:
                    # If this line doesn't start with # or > and it isn't just
                    # a blank line, then something's up. Throw an error.
                    stripped_line = line.strip()
                    if len(stripped_line) > 0:
                        raise WeirdError(
                            "Unrecognized line prefix in SCO: line = "
                            f'"{stripped_line}"'
                        )

    # shouldn't happen unless something really weird happens with the decoy
    # genome, but let's account for it anyway
    if len(genes) < 1:
        raise WeirdError("No genes described in SCO.")

    df = pd.DataFrame.from_dict(
        genes,
        orient="index",
        columns=["LeftEnd", "RightEnd", "Length", "Strand"],
    )
    return df


def get_prodigal_genes(seq, name):
    """Runs Prodigal on a sequence and returns info about the predicted genes.

    Parameters
    ----------
    seq: skbio.DNA
        Sequence in which we'll predict genes.

    name: str
        Name of this sequence. Just used for naming the temporary files; this
        shouldn't matter, but it could be useful for debugging.

    Returns
    -------
    df: pd.DataFrame
        Describes genes predicted in this sequence. See parse_sco()'s docs for
        details on this. (Importantly, the coordinates are 1-indexed and
        inclusive.)

    Raises
    ------
    subprocess.CalledProcessError
        If Prodigal has a problem. I imagine the most likely cause would be
        really short sequences; Prodigal will throw an error if the input
        sequence has < 20,000 characters, at least as of writing.

    Notes
    -----
    This assumes that the sequence is prokaryotic, since Prodigal is only
    designed for bacterial and archaeal genomes; see
    https://github.com/hyattpd/Prodigal/wiki/introduction.
    """
    with tempfile.TemporaryDirectory() as td:
        # Write out the sequence to a file
        # (yeah we could pipe it into prodigal but i don't trust myself to
        # do that correctly)
        fasta_fp = os.path.join(td, f"{name}.fasta")
        with open(fasta_fp, "w") as dffh:
            skbio.io.write(seq, format="fasta", into=dffh)

        sco_fp = os.path.join(td, f"{name}.sco")

        subprocess.run(
            ["prodigal", "-i", fasta_fp, "-o", sco_fp, "-f", "sco", "-c"],
            check=True,
        )

        # Read in and return the predicted genes
        return parse_sco(sco_fp)


def compute_decoy_contig_mut_rates(
    contigs,
    bcf_obj,
    thresh_type,
    thresh_vals,
    decoy_contig,
    decoy_contexts,
):
    """Computes mutation rates for a decoy contig at some threshold values.

    Parameters
    ----------
    contigs: str
        Filepath to a FASTA file containing contigs in which mutations were
        naively called. We'll only really use this to extract the decoy
        contig's sequence (it's probably easier to load it here then to rely on
        the caller to load it).

    bcf_obj: pysam.VariantFile
        Object describing a BCF file produced by strainFlye's naive calling.

    thresh_type: str
        Either "p" or "r", depending on which type of mutations were called in
        bcf_obj.

    thresh_vals: range
        Range of values of p or r (depending on thresh_type) at which to
        compute mutation rates. Must use a step size of 1.

    decoy_contig: str
        Name of a contig to compute mutation rates for.

    decoy_contexts: list of str
        Context-dependent mutation settings to apply in computing the mutation
        rates. Each entry should be contained in config.DECOY_CONTEXTS.
        Details on these contexts' meanings:

         - "Full": Compute mutation rates across the entire contig.

         - "CP2": Only consider positions that are 1) located in a single
                  predicted protein-coding gene and 2) are located in the
                  second codon position of this gene.

         - "Tv": Compute mutation rates based on treating potential
                 transversion mutations (A <-> C, G <-> T, A <-> T, C <-> G)
                 as a decoy.

         - "Nonsyn": Compute mutation rates based on treating potential
                     nonsynonymous mutations (for positions located in a single
                     predicted protein-coding gene) as a decoy.

         - "Nonsense": Like Nonsyn, but for nonsense mutations.

         - "CP2Tv": Tv, but only for positions in CP2.
         - "CP2Nonsyn": Nonsyn, but only for positions in CP2.
         - "CP2Nonsense": Nonsense, but only for positions in CP2.
         - "TvNonsyn": Tv + Nonsyn mutations.
         - "TvNonsense": Tv + Nonsense mutations.
         - "CP2TvNonsense": Tv + Nonsense, but only for positions in CP2.
         - "CP2TvNonsyn": Tv + Nonsyn, but only for positions in CP2.

        ... Note that we don't have any options for combining Nonsyn and
        Nonsense, because this wouldn't do anything (all nonsense mutations are
        by definition nonsynonymous). Arguably, CP2 + Nonsyn is also *almost*
        useless, but there is one possible "synonymous" CP2 mutation (TGA <->
        TAA both code for a stop codon) so we include that.

    Returns
    -------
    ctx2mr: dict
        Maps each entry in decoy_contexts to a list of mutation rates computed
        for the decoy contig using this context.

        Each list has the same dimensions as thresh_vals. So, the t-th
        entry in a list corresponds to the mutation rate computed for the
        decoy contig (using the current decoy context) based on naive calling
        using the t-th threshold value.

    Raises
    ------
    SequencingDataError
        If decoy_contig isn't in the contigs. (This should never happen if
        this function is called from run_estimate(), which should already have
        ensured that this is the case.)

    subprocess.CalledProcessError
        Raised if something goes wrong with Prodigal (e.g. the decoy contig is
        < 20,000 bp long). See get_prodigal_genes().

    References
    ----------
    See https://www.mun.ca/biology/scarr/Transitions_vs_Transversions.html for
    details on transversions.
    """
    decoy_seq = fasta_utils.get_single_seq(contigs, decoy_contig)

    # Figure out if we gotta run Prodigal on the decoy contig
    decoy_genes_df = None
    has_genic_decoy_context = False
    for ctx in decoy_contexts:
        if "CP2" in ctx or "Nonsyn" in ctx or "Nonsense" in ctx:
            has_genic_decoy_context = True
            break

    # Ok we gotta run Prodigal
    if has_genic_decoy_context:
        decoy_genes_df = get_prodigal_genes(decoy_seq, decoy_contig)

    ctx2mr = {}

    for ctx in decoy_contexts:
        if ctx == "Full":
            # PERF: In theory we could avoid calling get_single_seq() if we
            # JUST have "Full" as the decoy context, since we should already
            # know the length of this sequence from run_estimate() -- but eh
            # not a big deal probs
            ctx2mr[ctx] = compute_full_decoy_contig_mut_rates(
                bcf_obj,
                thresh_type,
                thresh_vals,
                decoy_contig,
                len(decoy_seq),
            )

        elif ctx == "CP2":
            ctx2mr[ctx] = compute_cp2_decoy_contig_mut_rates(
                bcf_obj,
                thresh_type,
                thresh_vals,
                decoy_contig,
                decoy_genes_df,
            )

        elif ctx == "Tv":
            raise NotImplementedError("oop")
        elif ctx == "Nonsyn":
            raise NotImplementedError("oop")
        elif ctx == "Nonsense":
            raise NotImplementedError("oop")
        elif ctx == "CP2Tv":
            # find cp2 positions (abstract to util func), limit to potential
            # tvs, identify mutations, end
            raise NotImplementedError("oop")
        elif ctx == "CP2Nonsyn":
            raise NotImplementedError("oop")
        elif ctx == "CP2Nonsense":
            raise NotImplementedError("oop")
        elif ctx == "TvNonsyn":
            raise NotImplementedError("oop")
        elif ctx == "TvNonsense":
            raise NotImplementedError("oop")
        elif ctx == "CP2TvNonsense":
            raise NotImplementedError("oop")
        elif ctx == "CP2TvNonsyn":
            raise NotImplementedError("oop")
        else:
            # It shouldn't be possible for the user to sneak this past Click,
            # but let's catch it anyway
            raise WeirdError(f"Unrecognized decoy context {ctx}.")

    return ctx2mr


def run_estimate(
    contigs,
    bcf,
    diversity_indices,
    decoy_contig,
    decoy_contexts,
    high_p,
    high_r,
    decoy_min_length,
    decoy_min_average_coverage,
    output_dir,
    fancylog,
    chunk_size=500,
):
    """Performs decoy selection and FDR estimation.

    Notably, both high_p and high_r will be defined regardless of if the BCF
    was generated using p- or r-mutations, because (unlike strainFlye call)
    I don't think it's worth splitting this step up into two sub-commands
    by p- or r-mutations. So we'll just ignore one of these values.

    Parameters
    ----------
    contigs: str
        Filepath to a FASTA file containing contigs in which mutations were
        naively called.

    bcf: str
        Filepath to a BCF file generated by one of strainFlye call's
        subcommands.

    diversity_indices: str or None
        If a str, this should be a filepath to a TSV file containing diversity
        index info, also generated by one of strainFlye call's subcommands.
        We'll use this information to automatically select a decoy contig.

    decoy_contig: str or None
        If a str, this should be the name of a contig described in the
        BCF file. We'll use this as a decoy contig.

    decoy_contexts: list of str
        Context-dependent mutation settings (e.g. Full, CP2, Nonsyn, ...)
        Thankfully, we know each entry in this list will be one of a set of
        allowed choices, since we use click.Choice() to screen it at the CLI.
        We'll generate one TSV file for each of these contexts.

    high_p: int
        "Indisputable" threshold for p-mutations (scaled up by 100).

    high_r: int
        "Indisputable" threshold for r-mutations.

    decoy_min_length: int
        If automatically selecting decoy contigs, we'll only consider contigs
        that are at least this long.

    decoy_min_average_coverage: float
        If automatically selecting decoy contigs, we'll only consider contigs
        with average coverages of at least this.

    output_dir: str

    fancylog: function
        Logging function.

    chunk_size: int
        After seeing this many target contigs, we'll write out their FDR and (#
        mutations per Mb) information to the corresponding files.

    Returns
    -------
    None

    Raises
    ------
    ParameterError
        - If the selected decoy contig isn't present in the contigs file.
        - If the high p (or high r) threshold is <= the minimum threshold value
          used in the BCF file.
        - If the BCF file's contigs do not exactly match the FASTA file's
          contigs.

    - parse_sf_bcf() can also raise various errors if the input BCF is
      malformed.
    - fasta_utils.get_name2len() can also raise errors if the input FASTA file
      of contigs is malformed.
    """
    fancylog("Loading and checking FASTA and BCF files...")

    # get name -> length mapping for the FASTA file; also sanity check it a bit
    contig_name2len = fasta_utils.get_name2len(contigs, min_num_contigs=2)

    # Load the BCF file, also
    bcf_obj, thresh_type, thresh_min = parse_sf_bcf(bcf)

    # Figure out which contigs are considered in the BCF file
    # (thankfully, this header can include contigs with no called mutations,
    # which makes my life easier here)
    bcf_contigs = set(bcf_obj.header.contigs)

    # Ensure that the sets of contigs in the BCF file and FASTA match exactly
    # (In theory, we could allow the BCF to be a subset of the FASTA, but...
    # nah, that's too much work and the user should already have an exactly-
    # matching FASTA file around from when they ran "call".)
    misc_utils.verify_contig_subset(
        bcf_contigs,
        set(contig_name2len),
        "the BCF file",
        "the FASTA file",
        exact=True,
    )
    misc_utils.verify_contig_lengths(contig_name2len, bcf_obj=bcf_obj)
    # We *could* try to ensure that the diversity index file's contigs, if
    # a diversity index file is specified, are a subset of the BCF -- but
    # no need to do this extra work right now. the main thing that matters IMO
    # is just checking that the selected decoy contig is in the BCF, which is
    # much easier to do later.
    fancylog(
        (
            "The FASTA and BCF files describe the same "
            f"{len(contig_name2len):,} contigs."
        ),
        prefix="",
    )
    fancylog(
        f"Also, the input BCF file contains {thresh_type}-mutations "
        f"(minimum {thresh_type} = {thresh_min:,}).",
        prefix="",
    )

    # Identify decoy contig
    selection_type = check_decoy_selection(diversity_indices, decoy_contig)
    if selection_type == "DI":
        fancylog("Selecting a decoy contig based on the diversity indices...")
        used_decoy_contig = autoselect_decoy(
            diversity_indices,
            decoy_min_length,
            decoy_min_average_coverage,
            fancylog,
        )
        fancylog(
            f"Selected {used_decoy_contig} as the decoy contig.", prefix=""
        )
    else:
        used_decoy_contig = decoy_contig
        fancylog(f"The specified decoy contig is {used_decoy_contig}.")

    # verify that the decoy contig is actually contained in the FASTA file
    if used_decoy_contig not in contig_name2len:
        raise ParameterError(
            f"Selected decoy contig {used_decoy_contig} is not present in "
            f"{contigs}."
        )
    fancylog(
        (
            "Verified that this decoy contig is present in the BCF and FASTA "
            "files."
        ),
        prefix="",
    )

    # if someone chuckles at this, the project is successful
    # that's how it works
    fancylog("(Sorry for doubting you.)", prefix="")

    # Figure out the exact p or r values we'll iterate through -- these will
    # correspond to the columns in our output TSV of FDR estimation (i.e.
    # for each target contig, we'll produce this many FDR estimates).
    #
    # NOTE 1: For the time being, we just go through in increments of 1 (for
    # p, this is 0.01%; for r, this is just 1 read). We could support other
    # "step" values, but that's a lot of work for probably little benefit
    # (unless this ends up being a bottleneck, idk).
    #
    # NOTE 2: We do not produce an estimate for the exact high_p (or high_r)
    # value. The maximum threshold is that minus the step value (... which is
    # always 1, at least right now).
    fancylog(f"Determining range of value(s) of {thresh_type} to consider...")
    if thresh_type == "p":
        if high_p <= thresh_min:
            raise ParameterError(
                f"--high-p = {high_p:,} must be larger than the minimum p "
                f"used in the BCF ({thresh_min:,})."
            )
        thresh_high = high_p
    else:
        if high_r <= thresh_min:
            raise ParameterError(
                f"--high-r = {high_r:,} must be larger than the minimum r "
                f"used in the BCF ({thresh_min:,})."
            )
        thresh_high = high_r
    thresh_max = thresh_high - 1
    thresh_vals = range(thresh_min, thresh_high)
    fancylog(
        (
            f"We'll consider {len(thresh_vals):,} value(s) of {thresh_type}: "
            f"from {thresh_min:,} to {thresh_max:,}."
        ),
        prefix="",
    )

    # Extra clarification about indisputable mutations. I want to avoid taking
    # people by surprise with this.
    fancylog(
        (
            f"{thresh_type}-mutations for {thresh_type} \u2265 "
            f'{thresh_high:,} will be considered "indisputable."'
        ),
        prefix="",
    )
    fancylog(
        (
            'These "indisputable" mutations won\'t be included in the FDR '
            "estimation results."
        ),
        prefix="",
    )

    # Click allows the user to specify the same Choice multiple times, which
    # will literally give us a list with the same thing multiple times. So
    # let's filter to unique decoy contexts, so that we don't generate the CP2
    # mutation rates a gazillion times for some weird reason...
    # (Also, let's sort this list so that there is a consistent order we can
    # rely on.)
    unique_ctxs = sorted(set(decoy_contexts))

    fancylog(
        f"Computing mutation rates for {used_decoy_contig} at these threshold "
        f"values, for each of the {len(unique_ctxs):,} decoy context(s)..."
    )
    # For each value in thresh_vals, compute the decoy genome's mutation rate.
    ctx2mr = compute_decoy_contig_mut_rates(
        contigs,
        bcf_obj,
        thresh_type,
        thresh_vals,
        used_decoy_contig,
        unique_ctxs,
    )
    fancylog("Done.", prefix="")

    # At this point, we've delayed this about as long as we can -- create the
    # output directory
    misc_utils.make_output_dir(output_dir)

    fancylog(
        "Computing mutation rates and FDR estimates for the "
        f"{len(contig_name2len) - 1:,} target contig(s)..."
    )

    # Create the header for the TSV files
    tsv_header = "Contig"
    for val in thresh_vals:
        tsv_header += f"\t{thresh_type}{val}"

    # ... and write it out. The header is the same for the FDR estimate file(s)
    # and for the (# of mutations per megabase) file.
    ctx2fp = {
        ctx: os.path.join(output_dir, f"fdr-{ctx}.tsv") for ctx in unique_ctxs
    }
    num_fp = os.path.join(output_dir, "num-mutations-per-mb.tsv")
    for tsv_fp in list(ctx2fp.values()) + [num_fp]:
        with open(tsv_fp, "w") as fdr_file:
            fdr_file.write(f"{tsv_header}\n")

    # Compute FDR estimates for each target contig.
    # This is analogous to the "Full" context-dependent option for the decoy
    # genome comptuation, so we can reuse a lot of code from that.
    target_contigs = bcf_contigs - {used_decoy_contig}

    ctx2fdr_out = {ctx: "" for ctx in unique_ctxs}
    num_out = ""
    # Sort target contigs so that their rows in the FDR / num-per-Mb files
    # are in lexicographic order. Not really needed, but nice to have and
    # makes testing easier
    for tc_ct, target_contig in enumerate(sorted(target_contigs), 1):
        ctx2fdr_lines, num_line = compute_target_contig_fdr_curve_info(
            bcf_obj,
            thresh_type,
            thresh_vals,
            target_contig,
            contig_name2len[target_contig],
            ctx2mr,
        )

        for ctx in ctx2mr:
            ctx2fdr_out[ctx] += ctx2fdr_lines[ctx]
        num_out += num_line

        # We'll "chunk" outputs -- we'll only perform a write operation
        # every (chunk_size) lines. This way, we don't need to perform
        # write operations after processing every target contig.
        if tc_ct % chunk_size == 0:

            for ctx in ctx2mr:
                with open(ctx2fp[ctx], "a") as ff:
                    ff.write(ctx2fdr_out[ctx])
                ctx2fdr_out[ctx] = ""

            with open(num_fp, "a") as nf:
                nf.write(num_out)
            num_out = ""

    # Make one last flush if needed (yeah, i know, i know, shouldn't reuse
    # code but blhufhaosdfu)
    if num_out != "":
        for ctx in ctx2mr:
            with open(ctx2fp[ctx], "a") as ff:
                ff.write(ctx2fdr_out[ctx])

        with open(num_fp, "a") as nf:
            nf.write(num_out)

    fancylog("Done.", prefix="")

    # TODO: Verify that the decoy contig has a nonzero mutation rate?
    # If not, that's problematic, because
    # then we'd estimate the FDR as zero for every target contig. That
    # shouldn't happen most of the time, anyway. Maybe add an option to limit
    # auto-selection to just contigs with mutations? Hm, but that would be
    # annoying to implement, and users can always manually set a decoy contig.


def get_optimal_threshold_values(fi, fdr):
    """Returns the optimal values of p or r for each contig's mutation calls.

    Parameters
    ----------
    fi: pd.DataFrame
        FDR information produced by "strainFlye fdr estimate". The indices
        (rows) correspond to contigs; the columns correspond to threshold
        values of p or r (sorted in ascending order from left to right). Cells
        indicate the estimated FDR for this contig at this threshold value (and
        may be NaN if no FDR estimate was defined for a given combination of
        (contig, threshold value)).

        This should have already been sanity-checked using
        load_and_sanity_check_fdr_file(), so we don't perform any validation on
        this DataFrame's structure here.

    fdr: float
        FDR at which (non-indisputable) mutation calls for each contig will be
        fixed.

    Returns
    -------
    pd.Series
        Has the same index as fi (so, this has one entry per contig). Each
        contig's entry will be the smallest threshold value (extracted
        from the column name -- i.e. "p15" will get converted to the int 15)
        at which this contig's FDR is less than or equal to the specified fdr.
        If no such threshold value exists (i.e. all estimated FDRs for this
        contig are greater than the specified fdr and/or are NaN), the contig's
        entry in this series will be None.
    """
    # For a given contig's FDR curve, we define four cases regarding how this
    # curve intersects with a fixed FDR (represented as a vertical line).
    # The goal is to, for each contig, identify the "optimal" value of p or r
    # that maximizes the number of mutations / mb we see while keeping the
    # estimated FDR below the fixed value.
    #
    # How do we identify this "optimal" value? By looking at the FDR curve.
    # We define four "cases" of how a FDR curve can look, relative to the fixed
    # FDR (which can be thought of as an infinite vertical line).
    #     |
    # Case 1. The curve crosses the line at exactly one point.
    #     |   This is easy to handle -- select the value of p or r just below
    #     |   the intersection for this contig.
    # ^  _|_____
    # | / |
    # |/  |
    # +---|------->
    #     |
    # Case 2. The curve crosses the line more than one point.
    #     |   (This is an unfortunate possibility, because we do not have the
    #     |   guarantee that FDR estimates increase monotonically with lower
    #     |   values of p or r.) In this case, we select the value of p or r
    #     |   just below the intersection for this contig with the highest
    #     |   # mutations / mb (in the plot below, at the top intersection).
    #     |
    #     |   Notably, this will always correspond to the lowest value of p or
    #     |   r that is <= the fixed FDR: lower values of p or r also "include"
    #     |   the p- or r-mutations called from higher values of p or r, so the
    #     |   lowest "passing" threshold value will also result in the highest
    #     |   number of mutations per megabase in the target contig.
    # ^  _|_____
    # | / |
    # |/  |
    # |\__|__
    # |  _|_/
    # | / |
    # |/  |
    # +---|------->
    #     |
    # Case 3. The curve crosses the line at zero points, and all estimated FDRs
    #     |   are LOWER than the fixed FDR. In this case, just select the
    #     |   lowest value of p or r used.
    #     |
    # ^ | |
    # | | |
    # |/  |
    # +---|------->
    #     |
    # Case 4. The curve crosses the line at zero points, and all estimated FDRs
    #     |   are HIGHER than the fixed FDR. In this case, our hands are tied;
    #     |   don't select any value of p or r. We can't call any
    #     |   non-indisputable mutations for this contig, at least not at this
    #     |   fixed FDR.
    #     |
    # ^   |    /
    # |   |   /
    # |   |  |
    # +---|------->

    # Convert the FDR information to a binary matrix:
    # True  means this FDR is <= the fixed FDR value
    # False means this FDR is >  the fixed FDR value (or is a NaN)
    #
    # Note that that the ~fi.isna() check (automatically rendering all NaNs as
    # False) isn't really required, since (per IEEE-754 standards, I think? --
    # see https://stackoverflow.com/a/1573715) running "NaN <= x" should yield
    # False for all x. However, I don't really want to rely on this implicit
    # condition, and I'll take making this 0.1 seconds slower if it means I
    # don't have to think about floating-point standards any more right now.
    lte_fdr = (~fi.isna()) & (fi <= fdr)

    # While we're at it, convert the columns of this DataFrame to their integer
    # values -- so, "p15" --> 15, for example. We should have already
    # sanity-checked this DataFrame's structure before calling this.
    lte_fdr.columns = fi.columns.str.slice(1).astype(int)

    def get_optimal_threshold(fdr_df_row):
        # We implicitly go through from lower to higher threshold values -- so
        # we'll select the lowest threshold value that is <= the fixed FDR.
        # This automatically accounts for Cases 1, 2, and 3 above.
        for ci, val in enumerate(fdr_df_row):
            if val:
                return fdr_df_row.index[ci]
        # If we make it here, then none of the threshold values were <= the
        # fixed FDR. So this is a "Case 4" situation -- return NaN.
        return np.nan

    # We can select each optimal threshold using DataFrame.apply(). This is
    # faster than naive looping through the DataFrame, but it could still
    # probably be sped up (although I'm not sure how exactly we could use
    # vectorization here). See
    # https://web.archive.org/web/20181106230656/https://engineering.upside.com/a-beginners-guide-to-optimizing-pandas-code-for-speed-c09ef2c6a4d6
    # for details.
    return lte_fdr.apply(get_optimal_threshold, axis=1)


def write_filtered_bcf(
    in_bcf_obj,
    out_bcf_fp,
    decoy_contig,
    optimal_thresh_vals,
    thresh_type,
    thresh_high,
    fancylog,
    verbose,
):
    """Writes out a filtered BCF based on optimal threshold values.

    The filtered BCF will include:

        1. "Indisputable" mutations (that pass thresh_high). We will include
           indisputable mutations from all contigs -- the target(s), and the
           decoy.

        2. Non-indisputable mutations (that do not pass thresh_high but pass
           the corresponding optimal_thresh_vals value for a target contig).
           We will only include these mutations from the target contigs, since
           we don't have an optimal threshold value generated for the decoy
           contig.

           If a target contig did not have an optimal threshold value given
           (i.e. its OTV is np.nan), then we will not include any
           non-indisputable mutations for this contig.

    Parameters
    ----------
    in_bcf_obj: pysam.VariantFile
        Object describing a BCF file produced by strainFlye's naive calling.

    out_bcf_fp: str
        Filepath to which a filtered version of in_bcf_obj will be written.

    decoy_contig: str
        Name of the decoy contig.

    optimal_thresh_vals: pd.Series
        Output of get_optimal_threshold_values(). The index corresponds to
        contig names; the values correspond to either the optimal value of p
        or r for a contig (expressed as a number), or np.nan (if no optimal
        value exists for this contig -- i.e. all of this contig's estimated
        FDRs were greater than the threshold or were undefined).

        Note that the dtype of this Series will probably be a float, because
        the presence of NaNs forces the dtype to be float. (This problem is
        documented, for example, at
        https://pandas.pydata.org/docs/user_guide/integer_na.html.) This isn't
        a problem, since I know that all of the numbers are integers (e.g.
        15.0) -- so we can call int() on them without worrying.

    thresh_type: str
        Either "p" or "r". We assume at this point that in_bcf_obj,
        optimal_thresh_vals, etc. all also use this thresh_type -- so please
        don't break this, ok?

    thresh_high: int
        The "high" (indisputable) value of p or r passed to
        "strainFlye fdr estimate".

    fancylog: function
        Logging function.

    verbose: bool
        If True, logs information about the number of mutations preserved for
        each contig.

    Returns
    -------
    None
    """
    # We will write out certain mutation "records" to this BCF.
    out_bcf_obj = pysam.VariantFile(out_bcf_fp, "wb", header=in_bcf_obj.header)

    # For each contig (considering both the decoy and target(s))...
    for contig in in_bcf_obj.header.contigs:

        num_written_muts = 0
        num_original_muts = 0

        thresh_used = thresh_high
        # We only call "indisputable" mutations for the decoy contig.
        if contig != decoy_contig:
            # Figure out the minimum threshold to use for this contig. If we
            # can only output indisputable mutations for this contig, then this
            # threshold will remain thresh_high; if we have a lower threshold
            # value defined, we can use that. (We can do this because mutations
            # that pass thresh_high will automatically also pass lower
            # thresholds.)
            otv = optimal_thresh_vals[contig]
            if not np.isnan(otv):
                thresh_used = otv

        for mut in in_bcf_obj.fetch(contig):

            alt_pos = mut.info.get("AAD")[0]
            cov_pos = mut.info.get("MDP")

            # We can figure out whether or not this mutation passes thresh_used
            # by using the calling functions from call_utils. Notably, we pass
            # 1 as min_alt_pos for call_p_mutation() because we don't care
            # about min_alt_pos at this point -- like, it applied to the
            # mutations the user screened for using "strainFlye call"
            is_mut = False
            if thresh_type == "p":
                is_mut = call_utils.call_p_mutation(
                    alt_pos, cov_pos, thresh_used, 1
                )
            else:
                is_mut = call_utils.call_r_mutation(alt_pos, thresh_used)

            if is_mut:
                out_bcf_obj.write(mut)
                num_written_muts += 1

            num_original_muts += 1

        if verbose:
            fancylog(
                (
                    f"Wrote out {num_written_muts:,} / {num_original_muts:,} "
                    f"mutations for {contig}."
                ),
                prefix="",
            )


def load_and_sanity_check_fdr_file(fdr_info, thresh_type):
    """Loads and sanity-checks a FDR TSV file.

    This sanity-checks the structure of the file -- it doesn't say anything
    about the contigs all matching up with a BCF file, for example. The main
    goal is just checking that this looks like the sort of TSV file that the
    estimate command would have generated.

    Parameters
    ----------
    fdr_info: str
        Filepath to a FDR TSV file.

    thresh_type: str
        Either "p" or "r", depending on which type of mutations were called.

    Returns
    -------
    fi: pd.DataFrame
        DataFrame containing the information stored in the TSV file. Indices
        (rows) correspond to contigs; columns correspond to threshold values
        of p or r (sorted in ascending order from left to right). Cells
        indicate the estimated FDR for this contig at this threshold value.

    Raises
    ------
    ParameterError
        If the TSV file seems malformed.

    Other errors
        Can be raised by pd.read_csv() if the file seems malformed.
        We don't attempt to catch these errors.
    """
    fi = pd.read_csv(fdr_info, sep="\t", index_col=0)
    # error prefix -- saves us some typing...
    ep = "Input FDR TSV file seems malformed"
    if fi.index.name != "Contig":
        raise ParameterError(f'{ep}: no "Contig" header?')
    if len(fi.index) < 1:
        raise ParameterError(f"{ep}: no contigs described?")
    if len(fi.columns) < 1:
        raise ParameterError(f"{ep}: no threshold values described?")

    prev_col_val = None
    for col in fi.columns:
        if col[0] != thresh_type:
            raise ParameterError(
                f"{ep}: columns should start with {thresh_type}."
            )
        col_val = int(col[1:])
        if prev_col_val is not None:
            if col_val <= prev_col_val:
                raise ParameterError(
                    f"{ep}: values of {thresh_type} should increase from left "
                    "to right."
                )
            # We could eventually support different step sizes, but not yet.
            # Actually I think this might be overzealous -- I don't think we do
            # anything in "fix" that requires known step sizes -- but whatever,
            # I wanna be careful.
            if col_val != prev_col_val + 1:
                raise ParameterError(
                    f"{ep}: values of {thresh_type} should only increase in "
                    "steps of 1."
                )
        prev_col_val = col_val

        col_fdrs = fi[col]
        # Check that this column is numeric (NaNs are ok).
        # See https://stackoverflow.com/a/45568283.
        if not pd.api.types.is_numeric_dtype(col_fdrs):
            raise ParameterError(
                f"{ep}: Column {col} doesn't seem to be numeric?"
            )

        # At this point, we've already screened for non-numeric dtypes, so we
        # should be able to use < 0 without problems
        if (col_fdrs < 0).any():
            raise ParameterError(
                f"{ep}: Column {col} contains negative FDR estimates?"
            )

    return fi


def log_optimal_threshold_value_stats(
    optimal_thresh_vals, thresh_type, thresh_min, thresh_max, fdr, fancylog
):
    """Logs information about the optimal threshold values.

    Selecting these values is arguably the main part of how "strainFlye fdr
    fix" works, so I wanted to be as clear as possible -- hence this function
    existing.

    Parameters
    ----------
    optimal_thresh_vals: pd.Series
        Output of get_optimal_threshold_values(). The index corresponds to
        contig names; the values correspond to either the optimal value of p
        or r for a contig (expressed as a number), or np.nan (if no optimal
        value exists for this contig -- i.e. all of this contig's estimated
        FDRs were greater than the threshold or were undefined).

        Note that the dtype of this Series will probably be a float, because
        the presence of NaNs forces the dtype to be float. (This problem is
        documented, for example, at
        https://pandas.pydata.org/docs/user_guide/integer_na.html.) This isn't
        a problem, since I know that all of the numbers are integers (e.g.
        15.0) -- so we can call int() on them without worrying.

    thresh_type: str
        Either "p" or "r".

    thresh_min: int
        Minimum value of p or r used in "strainFlye call".

    thresh_max: int
        The maximum value of p or r: this is equal to the "high" (indisputable)
        value of p or r passed to "strainFlye fdr estimate" minus 1. Note that
        this can technically be equal to thresh_min in the silly case where
        --high-p = --min-p + 1 (same goes for r), although that really
        shouldn't happen in practice.

    fdr: float
        FDR at which (non-indisputable) mutation calls for each contig will be
        fixed.

    fancylog: function
        Logging function.

    Returns
    -------
    None
    """
    # A contig can have a N/A optimal threshold value if its entire FDR curve
    # was > the fixed FDR (see Case 4 in get_optimal_threshold_values()).
    nonna = optimal_thresh_vals[~optimal_thresh_vals.isna()]
    num_nonna = len(nonna)
    num_targets = len(optimal_thresh_vals)

    if num_nonna == 0:
        # This should not happen except for really weird cases
        fancylog(
            (
                f"Warning: No values of {thresh_type} resulted in estimated "
                f"FDRs \u2264 the fixed FDR, for all {num_targets:,} contigs."
            ),
            prefix="",
        )
    else:
        fancylog(
            (
                f"For {num_nonna:,} / {num_targets:,} contigs, there exist "
                f"values of {thresh_type} (at least, considering the range "
                f"from {thresh_type} = {thresh_min} to {thresh_type} = "
                f"{thresh_max}) that yield estimated FDRs \u2264 {fdr}%."
            ),
            prefix="",
        )
        # NOTE: Could roll these into a single loop to speed this up, but this
        # almost certainly won't be a bottleneck
        # Also see above re: int() calls -- all of the numbers here should ints
        # that just end with ".0", so we're not actually losing any information
        min_contig = nonna.idxmin()
        min_tv = int(nonna[min_contig])
        max_contig = nonna.idxmax()
        max_tv = int(nonna[max_contig])
        mean_tv = mean(nonna)
        fancylog(
            (
                f"These values range from {thresh_type} = {min_tv:,} "
                f"({min_contig}) to {thresh_type} = {max_tv:,} ({max_contig})."
            ),
            prefix="",
        )
        fancylog(
            f"The mean of these values is {thresh_type} = {mean_tv:,.2f}.",
            prefix="",
        )


def run_fix(bcf, fdr_info, fdr, output_bcf, fancylog, verbose):
    """Performs FDR fixing.

    Parameters
    ----------
    bcf: str
        Filepath to a BCF file generated by one of strainFlye call's
        subcommands.

    fdr_info: str
        Filepath to a TSV file describing estimated FDRs for the target
        contigs.

    fdr: int
        False Discovery Rate (FDR) to fix mutation calls at. Scaled up by 100,
        like values of p are elsewhere -- so fdr = 100 indicates an FDR of 1%.

    output_bcf: str
        Filepath to which we will write an (indexed) BCF file. This will
        contain a subset of the mutations described in the input BCF file.

    fancylog: function
        Logging function.

    verbose: bool
        If True, display information about each contig while writing the
        filtered BCF. At least on the SheepGut dataset, this step takes the
        longest, and it can be useful to allow verbosity there.

    Returns
    -------
    None

    Raises
    ------
    ParameterError
        - If the set of contigs in the FDR TSV file is not a subset of those
          in the BCF file.
        - If there is not exactly one contig that is present in the BCF file
          but not in the FDR TSV file.

    - parse_sf_bcf() can also raise various errors if the input BCF is
      malformed.
    """
    fancylog("Loading and checking BCF and TSV files...")
    # Like in run_estimate(): Load the BCF file and figure out what contigs it
    # describes
    bcf_obj, thresh_type, thresh_min = parse_sf_bcf(bcf)
    bcf_contigs = set(bcf_obj.header.contigs)

    # Load the estimated FDR file.
    fi = load_and_sanity_check_fdr_file(fdr_info, thresh_type)
    tsv_desc = "the FDR TSV file"
    # Ensure that the contigs described in these TSV files (tsv_contigs)
    # are all described in the BCF file. We don't check for an exact match,
    # because the decoy contig will be missing.
    tsv_contigs = set(fi.index)
    misc_utils.verify_contig_subset(
        tsv_contigs, bcf_contigs, tsv_desc, "the BCF file"
    )
    absent_contigs = bcf_contigs - tsv_contigs
    if len(absent_contigs) != 1:
        # Fun thing about this error message: we can say "contigs" because this
        # error fires if and only if len(absent_contigs) is not 1 :D
        raise ParameterError(
            "Exactly one contig in the BCF file (the decoy contig) should be "
            f"missing from {tsv_desc}. However, {len(absent_contigs):,} "
            "contigs are missing!"
        )

    # If we've made it here, we know there is exactly one contig in the BCF but
    # not in the FDR TSV file. This must be the decoy contig.
    decoy_contig = absent_contigs.pop()
    fancylog(
        f"Looks good so far; decoy contig seems to be {decoy_contig}.",
        prefix="",
    )

    # Figure out the "indisputable" mutation cutoff.
    # At this point, we already know that the FDR info file is structured as
    # expected (e.g. columns are all formatted like "p15", "p16", etc.), so we
    # can slice off the first character from the last column without worrying
    # about the file being structured incorrectly.
    thresh_max = int(fi.columns[-1][1:])
    thresh_high = thresh_max + 1
    fancylog(
        (
            'Looks like the cutoff for "indisputable" mutations was '
            f"{thresh_type} = {thresh_high:,}."
        ),
        prefix="",
    )
    fancylog(
        (
            "All mutations passing this cutoff will be included in the "
            "output BCF file."
        ),
        prefix="",
    )

    fancylog(
        "Based on the FDR information, finding optimal values of "
        f"{thresh_type} for each contig..."
    )
    # We can now begin in earnest. Figure out the "optimal" value of p or r for
    # for each contig, based on the estimated FDR information.
    optimal_thresh_vals = get_optimal_threshold_values(fi, fdr)
    fancylog("Done.", prefix="")
    log_optimal_threshold_value_stats(
        optimal_thresh_vals, thresh_type, thresh_min, thresh_max, fdr, fancylog
    )

    # VCF / BCF headers aren't (currently) editable using pysam
    # (https://github.com/pysam-developers/pysam/issues/668), but we can
    # fortunately use "bcftools annotate" to do this the long(ish) way -- write
    # the "first" BCF file to a temporary file, then annotate it, and output
    # the annotated BCF to the final "output BCF" location. (Annotation doesn't
    # seem to be doable "in place" using bcftools.)
    with tempfile.NamedTemporaryFile(
        mode="w+b"
    ) as temp_bcf_file, tempfile.NamedTemporaryFile(mode="w+") as header_file:
        fancylog(
            "Writing a filtered BCF file (to a temporary location, for now) "
            "including both "
            "(1) indisputable mutations from all contigs and "
            "(2) non-indisputable mutations from the target contigs that "
            f"result in a FDR \u2264 {fdr}%..."
        )
        # Filter mutations for each contig to those that pass these thresholds
        # (in addition to indisputable mutations that pass thresh_high).
        write_filtered_bcf(
            bcf_obj,
            temp_bcf_file.name,
            decoy_contig,
            optimal_thresh_vals,
            thresh_type,
            thresh_high,
            fancylog,
            verbose,
        )
        fancylog("Done.", prefix="")

        fancylog(
            "Updating the filtered BCF file's header to indicate that FDR "
            "fixing was done..."
        )
        # We don't have information about the decoy context (Full, CP2, ...)
        # but that's not a big deal. the main thing is just making it clear
        # that this file has been updated from the naive calls
        new_header_line = (
            f"##FILTER=<ID=strainFlye_fdr_fix_fdr_{fdr}_decoy_{decoy_contig}_"
            f'high_{thresh_type}_{thresh_high}, description="Mutations '
            'filtered to a fixed FDR">'
        )
        with open(header_file.name, "w") as hf:
            hf.write(new_header_line)
        # Update the header in the temp BCF file, and -- while this is
        # happening -- write out the updated BCF file to the output location
        subprocess.run(
            [
                "bcftools",
                "annotate",
                "-h",
                header_file.name,
                "-O",
                "b",
                "-o",
                output_bcf,
                temp_bcf_file.name,
            ],
            check=True,
        )
        fancylog("Done.", prefix="")

        # just gotta index the BCF, then we're done!
        bcf_utils.index_bcf(output_bcf, fancylog)
